###########################
# config/settings.py
###########################
# Dependencies
from typing import Set
from pathlib import Path
from config.constants import MetricType
from pydantic_settings import BaseSettings
from pydantic_settings import SettingsConfigDict


class Settings(BaseSettings):
    """
    Application settings with environment variable support
    """
    model_config                   = SettingsConfigDict(env_file          = '.env',
                                                        env_file_encoding = 'utf-8',
                                                        case_sensitive    = False,
                                                       )
    
    # Application
    APP_NAME            : str      = "AI Image Screener"
    VERSION             : str      = "1.0.0"
    DEBUG               : bool     = False
    LOG_LEVEL           : str      = "INFO"

    # Server Configuration
    HOST                : str      = "localhost"
    PORT                : int      = 8005
    WORKERS             : int      = 4
    
    # File processing 
    MAX_FILE_SIZE_MB    : int      = 10
    MAX_BATCH_SIZE      : int      = 50
    ALLOWED_EXTENSIONS  : Set[str] = {".jpg", ".jpeg", ".png", ".webp"}
    
    # Detection thresholds
    REVIEW_THRESHOLD    : float    = 0.65
    
    # Metric weights (must sum to 1.0)
    GRADIENT_WEIGHT     : float    = 0.30
    FREQUENCY_WEIGHT    : float    = 0.25
    NOISE_WEIGHT        : float    = 0.20
    TEXTURE_WEIGHT      : float    = 0.15
    COLOR_WEIGHT        : float    = 0.10
    
    # Processing
    ENABLE_CACHING      : bool     = True
    PROCESSING_TIMEOUT  : int      = 30
    PARALLEL_PROCESSING : bool     = True
    MAX_WORKERS         : int      = 4
    
    # Paths
    BASE_DIR            : Path     = Path(__file__).parent.parent
    UPLOAD_DIR          : Path     = BASE_DIR / "data" / "uploads"
    REPORTS_DIR         : Path     = BASE_DIR / "data" / "reports"
    CACHE_DIR           : Path     = BASE_DIR / "data" / "cache"
    LOGS_DIR            : Path     = BASE_DIR / "logs"
    
    

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._create_directories()
        self._validate_weights()

    
    def _create_directories(self):
        """
        Ensure all required directories exist
        """
        for directory in [self.UPLOAD_DIR, self.REPORTS_DIR, self.CACHE_DIR, self.LOGS_DIR]:
            directory.mkdir(parents  = True, 
                            exist_ok = True,
                           )
    
    def _validate_weights(self):
        """
        Validate metric weights sum to 1.0
        """
        total = (self.GRADIENT_WEIGHT +
                 self.FREQUENCY_WEIGHT +
                 self.NOISE_WEIGHT +
                 self.TEXTURE_WEIGHT +
                 self.COLOR_WEIGHT
                )

        if (not (0.99 <= total <= 1.01)):
            raise ValueError(f"Metric weights must sum to 1.0, got {total}")
    

    @property
    def max_file_size_bytes(self) -> int:
        return self.MAX_FILE_SIZE_MB * 1024 * 1024
    

    def get_metric_weights(self) -> dict:
        """
        Get all metric weights as dictionary
        """
        return {MetricType.GRADIENT  : self.GRADIENT_WEIGHT,
                MetricType.FREQUENCY : self.FREQUENCY_WEIGHT,
                MetricType.NOISE     : self.NOISE_WEIGHT,
                MetricType.TEXTURE   : self.TEXTURE_WEIGHT,
                MetricType.COLOR     : self.COLOR_WEIGHT
               }


# Singleton 
settings = Settings()


###########################
# config/constants.py
###########################
# Dependencies
from enum import Enum
from dataclasses import dataclass


class DetectionStatus(str, Enum):
    """
    Overall detection status
    """
    LIKELY_AUTHENTIC = "LIKELY_AUTHENTIC"
    REVIEW_REQUIRED  = "REVIEW_REQUIRED"


class SignalStatus(str, Enum):
    """
    Individual signal status
    """
    PASSED  = "passed"
    WARNING = "warning"
    FLAGGED = "flagged"


class FileFormat(str, Enum):
    """
    Supported file formats
    """
    JPG  = ".jpg"
    JPEG = ".jpeg"
    PNG  = ".png"
    WEBP = ".webp"


class MetricType(str, Enum):
    """
    Detection metric types
    """
    GRADIENT  = "gradient"
    FREQUENCY = "frequency"
    NOISE     = "noise"
    TEXTURE   = "texture"
    COLOR     = "color"



# Signal thresholds
SIGNAL_THRESHOLDS          = {SignalStatus.FLAGGED : 0.7,
                              SignalStatus.WARNING : 0.4,
                              SignalStatus.PASSED  : 0.0,
                             }

# Metric explanations
METRIC_EXPLANATIONS        = {MetricType.GRADIENT  : {'high'     : "Detected irregular gradient patterns typical of diffusion models. Natural photos show consistent lighting gradients shaped by physics.",
                                                      'moderate' : "Some gradient inconsistencies detected. May indicate AI generation or heavy editing.",
                                                      'normal'   : "Gradient patterns are consistent with natural lighting and camera optics."
                                                     },
                              MetricType.FREQUENCY : {'high'     : "Unusual frequency distribution detected. AI-generated images often show unnatural spectral patterns.",
                                                      'moderate' : "Frequency patterns show some irregularities. Requires further review.",
                                                      'normal'   : "Frequency distribution matches expected patterns for authentic photographs."
                                                     },
                              MetricType.NOISE     : {'high'     : "Noise pattern is unnaturally uniform. Real camera sensors produce characteristic noise patterns.",
                                                      'moderate' : "Noise distribution shows some anomalies. May indicate synthetic generation.",
                                                      'normal'   : "Noise characteristics are consistent with genuine camera sensor behavior."
                                                     },
                              MetricType.TEXTURE   : {'high'     : "Detected suspiciously smooth regions. Natural photos have organic texture variation.",
                                                      'moderate' : "Some texture regions appear overly uniform. Further analysis recommended.",
                                                      'normal'   : "Texture variation is within expected ranges for authentic photographs."
                                                     },
                              MetricType.COLOR     : {'high'     : "Color distribution shows impossible or highly unlikely patterns.",
                                                      'moderate' : "Some color histogram irregularities detected.",
                                                      'normal'   : "Color distribution is within normal ranges for real photographs."
                                                     }
                             }

# Basic Image Processing Constants
MIN_IMAGE_DIMENSION        = 64
MAX_IMAGE_DIMENSION        = 8192
LUMINANCE_WEIGHTS          = (0.2126, 0.7152, 0.0722)  # ITU-R BT.709
IMAGE_RESIZE_MAX_DIMENSION = 1024


# Gradient-Field PCA Detection Parameters
@dataclass(frozen = True)
class GradientFieldPCAParams:
    """
    Parameters for Gradient-Field PCA detection
    """
    # Random Seed For Reproducibility 
    RANDOM_SEED                : int   = 1234

    # NEUTRAL_SCORE
    NEUTRAL_SCORE              : float = 0.5

    # PCA Configuration
    SAMPLE_SIZE                : int   = 10000  # Max gradient samples for PCA
    
    # Thresholds
    MAGNITUDE_THRESHOLD        : float = 1e-6   # Minimum gradient magnitude to consider
    MIN_SAMPLES                : int   = 10     # Minimum samples required for PCA
    VARIANCE_THRESHOLD         : float = 1e-10  # Minimum total variance
    EIGENVALUE_RATIO_THRESHOLD : float = 0.85   # Real photos typically > 0.85



# Frequency Analysis Parameters
@dataclass(frozen = True)
class FrequencyAnalysisParams:
    """
    Parameters for FFT-based frequency analysis
    """
    # NEUTRAL_SCORE
    NEUTRAL_SCORE       : float = 0.5

    # FFT Configuration
    BINS                : int   = 64
    HIGH_FREQ_THRESHOLD : float = 0.6     # Radial position where high-freq starts
    
    # Analysis Thresholds
    MIN_SPECTRUM_SAMPLES : int   = 10
    HF_RATIO_UPPER       : float = 0.35   # High-frequency ratio upper bound
    HF_RATIO_LOWER       : float = 0.08   # High-frequency ratio lower bound
    
    # Scaling Factors
    HF_UPPER_SCALE       : float = 3.0
    HF_LOWER_SCALE       : float = 5.0
    ROUGHNESS_SCALE      : float = 10.0
    DEVIATION_SCALE      : float = 2.0
    
    # Sub-metric Weights
    SUBMETRIC_WEIGHTS    : dict  = None
    
    def __post_init__(self):
        if self.SUBMETRIC_WEIGHTS is None:
            object.__setattr__(self, 'SUBMETRIC_WEIGHTS', {'hf_anomaly' : 0.4,
                                                           'roughness'  : 0.3,
                                                           'deviation'  : 0.3,
                                                          }
                              )


# Noise Analysis Parameters
@dataclass(frozen = True)
class NoiseAnalysisParams:
    """
    Parameters for noise pattern analysis
    """
    # NEUTRAL SCORE 
    NEUTRAL_SCORE            : float = 0.5

    # Patch Configuration
    PATCH_SIZE               : int   = 32
    STRIDE                   : int   = 16
    SAMPLES                  : int   = 100
    
    # Variance Thresholds
    VARIANCE_LOW_THRESHOLD   : float = 1.0     # Skip too uniform patches
    VARIANCE_HIGH_THRESHOLD  : float = 1000.0  # Skip too structured patches
    
    # MAD Conversion
    MAD_TO_STD_FACTOR        : float = 1.4826  # Gaussian: σ ≈ 1.4826 × MAD
    
    # Distribution Analysis
    MIN_ESTIMATES            : int   = 10
    MIN_FILTERED_SAMPLES     : int   = 5
    OUTLIER_PERCENTILE_LOW   : int   = 10
    OUTLIER_PERCENTILE_HIGH  : int   = 90
    
    # CV (Coefficient of Variation) Thresholds
    CV_UNIFORM_THRESHOLD     : float = 0.15
    CV_VARIABLE_THRESHOLD    : float = 1.2
    CV_UNIFORM_SCALE         : float = 5.0
    CV_VARIABLE_SCALE        : float = 2.0
    
    # Noise Level Thresholds
    LEVEL_CLEAN_THRESHOLD    : float = 1.5
    LEVEL_LOW_THRESHOLD      : float = 2.5
    
    # IQR Analysis
    IQR_THRESHOLD            : float = 0.3
    IQR_SCALE                : float = 2.0
    IQR_PERCENTILE_LOW       : int   = 25
    IQR_PERCENTILE_HIGH      : int   = 75
    
    # Sub-metric Weights
    SUBMETRIC_WEIGHTS        : dict  = None
    
    def __post_init__(self):
        if self.SUBMETRIC_WEIGHTS is None:
            object.__setattr__(self, 'SUBMETRIC_WEIGHTS', {'cv_anomaly'          : 0.4,
                                                           'noise_level_anomaly' : 0.4,
                                                           'iqr_anomaly'         : 0.2,
                                                          }
                              )


# Texture Analysis Parameters
@dataclass(frozen = True)
class TextureAnalysisParams:
    """
    Parameters for texture analysis
    """
    # Random Seed for reproducibility
    RANDOM_SEED                : int   = 1234

    # Neutral Score 
    NEUTRAL_SCORE              : float = 0.5

    # Patch Configuration
    PATCH_SIZE                 : int   = 64
    N_PATCHES                  : int   = 50
    
    # Histogram Configuration
    HISTOGRAM_BINS             : int   = 32
    HISTOGRAM_RANGE            : tuple = (0, 255)
    
    # Edge Detection
    EDGE_THRESHOLD             : float = 10.0
    
    # Smoothness Analysis
    SMOOTHNESS_THRESHOLD       : float = 0.5
    SMOOTH_RATIO_THRESHOLD     : float = 0.4
    SMOOTH_RATIO_SCALE         : float = 2.5
    
    # Entropy Analysis
    ENTROPY_CV_THRESHOLD       : float = 0.15
    ENTROPY_SCALE              : float = 5.0
    
    # Contrast Analysis
    CONTRAST_CV_LOW            : float = 0.3
    CONTRAST_CV_HIGH           : float = 1.5
    CONTRAST_LOW_SCALE         : float = 2.0
    CONTRAST_HIGH_SCALE        : float = 0.5
    
    # Edge Density Analysis
    EDGE_CV_THRESHOLD          : float = 0.4
    EDGE_SCALE                 : float = 1.5
    
    # Sub-metric Weights
    SUBMETRIC_WEIGHTS          : dict  = None
    
    def __post_init__(self):
        if self.SUBMETRIC_WEIGHTS is None:
            object.__setattr__(self, 'SUBMETRIC_WEIGHTS', {'smoothness_anomaly' : 0.35,
                                                           'entropy_anomaly'    : 0.25,
                                                           'contrast_anomaly'   : 0.25,
                                                           'edge_anomaly'       : 0.15,
                                                          }
                              )


# Color Analysis Parameters
@dataclass(frozen = True)
class ColorAnalysisParams:
    """
    Parameters for color distribution analysis
    """
    # Random Seed for reproducibility
    RANDOM_SEED                  : int   = 1234

    # Neutral Score 
    NEUTRAL_SCORE                : float = 0.5
    # Saturation Analysis
    SAT_HIGH_THRESHOLD           : float = 0.8
    SAT_VERY_HIGH_THRESHOLD      : float = 0.95
    SAT_MEAN_THRESHOLD           : float = 0.65
    SAT_MEAN_SCALE               : float = 3.0
    HIGH_SAT_RATIO_THRESHOLD     : float = 0.20
    HIGH_SAT_SCALE               : float = 2.5
    CLIP_RATIO_THRESHOLD         : float = 0.05
    CLIP_SCALE                   : float = 10.0
    
    # Histogram Analysis
    HISTOGRAM_BINS               : int   = 64
    HISTOGRAM_RANGE              : tuple = (0, 1)
    ROUGHNESS_THRESHOLD          : float = 0.015
    ROUGHNESS_SCALE              : float = 50.0
    CLIP_THRESHOLD               : float = 0.10
    CLIP_SCALE_FACTOR            : float = 5.0
    
    # Hue Analysis
    HUE_SAT_MASK_THRESHOLD       : float = 0.2
    HUE_MIN_PIXELS               : int   = 100
    HUE_BINS                     : int   = 36
    HUE_RANGE                    : tuple = (0, 360)
    HUE_CONCENTRATION_THRESHOLD  : float = 0.6
    HUE_CONCENTRATION_SCALE      : float = 2.5
    HUE_EMPTY_BIN_THRESHOLD      : float = 0.01
    HUE_GAP_RATIO_THRESHOLD      : float = 0.4
    HUE_GAP_SCALE                : float = 1.5
    
    # Sub-metric Weights
    SAT_SUBMETRIC_WEIGHTS        : dict  = None
    HUE_SUBMETRIC_WEIGHTS        : dict  = None
    MAIN_WEIGHTS                 : dict  = None
    
    def __post_init__(self):
        if self.SAT_SUBMETRIC_WEIGHTS is None:
            object.__setattr__(self, 'SAT_SUBMETRIC_WEIGHTS', {'mean_anomaly'     : 0.3,
                                                               'high_sat_anomaly' : 0.4,
                                                               'clip_anomaly'     : 0.3,
                                                              }
                              )

        if self.HUE_SUBMETRIC_WEIGHTS is None:
            object.__setattr__(self, 'HUE_SUBMETRIC_WEIGHTS', {'concentration_anomaly' : 0.6,
                                                               'gap_anomaly'           : 0.4,
                                                              }
                              )

        if self.MAIN_WEIGHTS is None:
            object.__setattr__(self, 'MAIN_WEIGHTS', {'saturation' : 0.4,
                                                      'histogram'  : 0.35,
                                                      'hue'        : 0.25,
                                                     }
                              )



# Singleton instances for parameter classes
GRADIENT_FIELD_PCA_PARAMS = GradientFieldPCAParams()
FREQUENCY_ANALYSIS_PARAMS = FrequencyAnalysisParams()
NOISE_ANALYSIS_PARAMS     = NoiseAnalysisParams()
TEXTURE_ANALYSIS_PARAMS   = TextureAnalysisParams()
COLOR_ANALYSIS_PARAMS     = ColorAnalysisParams()


############################
# config/schemas.py
############################
# Dependencies
from typing import List
from typing import Dict
from pydantic import Field
from typing import Optional
from datetime import datetime
from pydantic import BaseModel
from config.constants import MetricType
from config.constants import SignalStatus
from config.constants import DetectionStatus


class MetricResult(BaseModel):
    """
    Raw metric output for explainability and reporting
    """
    metric_type : MetricType
    score       : float           = Field(..., ge = 0.0, le = 1.0)
    confidence  : Optional[float] = Field(None, ge = 0.0, le = 1.0)
    details     : Optional[Dict]  = Field(default_factory = dict)

    model_config                  = {"json_schema_extra" : {"example" : {"metric_type" : "noise",
                                                                         "score"       : 0.72,
                                                                         "confidence"  : 0.81,
                                                                         "details"     : {"patches_total" : 100,
                                                                                          "patches_valid" : 42,
                                                                                          "mean_noise"    : 1.12,
                                                                                          "cv"            : 0.18
                                                                                         }
                                                                        }
                                                           }
                                    }


class DetectionSignal(BaseModel):
    """
    Individual detection signal result
    """
    name        : str          = Field(..., description = "Metric name")
    metric_type : MetricType
    score       : float        = Field(..., ge = 0.0, le = 1.0, description = "Suspicion score (0=natural, 1=suspicious)")
    status      : SignalStatus
    explanation : str          = Field(..., description = "Human-readable explanation")
    
    model_config               = {"json_schema_extra" : {"example" : {"name"        : "Gradient Pattern",
                                                                      "metric_type" : "gradient",
                                                                      "score"       : 0.73,
                                                                      "status"      : "flagged",
                                                                      "explanation" : "Detected irregular gradient patterns typical of diffusion models."
                                                                     }
                                                        } 
                                 }


class AnalysisResult(BaseModel):
    """
    Single image analysis result
    """
    filename        : str
    overall_score   : float                          = Field(..., ge = 0.0, le = 1.0)
    status          : DetectionStatus
    confidence      : int                            = Field(..., ge = 0, le = 100, description = "Confidence percentage")
    signals         : List[DetectionSignal]
    metric_results  : Dict[MetricType, MetricResult]
    processing_time : float                          = Field(..., description = "Processing time in seconds")
    timestamp       : datetime                       = Field(default_factory = datetime.now)
    image_size      : tuple[int, int]                = Field(..., description = "Width x Height")
    
    model_config                                     =  {"json_schema_extra" : {"example" : {"filename"        : "photo_001.jpg",
                                                                                             "overall_score"   : 0.73,
                                                                                             "status"          : "REVIEW_REQUIRED",
                                                                                             "confidence"      : 73,
                                                                                             "signals"         : [],
                                                                                             "processing_time" : 2.34,
                                                                                             "image_size"      : [1920, 1080]
                                                                                            }
                                                                               }
                                                        }


class BatchAnalysisResult(BaseModel):
    """
    Batch analysis result
    """
    total_images          : int
    processed             : int
    failed                : int
    results               : List[AnalysisResult]
    summary               : Dict[str, int]       = Field(default_factory = dict, description = "Summary statistics")
    total_processing_time : float
    timestamp             : datetime             = Field(default_factory = datetime.now)


class APIResponse(BaseModel):
    """
    Standard API response wrapper
    """
    success   : bool
    message   : str
    data      : Optional[Dict] = None
    error     : Optional[str]  = None
    timestamp : datetime       = Field(default_factory = datetime.now)


class HealthResponse(BaseModel):
    """
    Health check response
    """
    status    : str
    version   : str
    uptime    : float
    timestamp : datetime = Field(default_factory = datetime.now)


######################
# utils/logger.py
######################

# Dependencies
import sys
import logging
from datetime import datetime
from config.settings import settings


class ColoredFormatter(logging.Formatter):
    """
    Colored log formatter for better readability
    """
    COLORS = {'DEBUG'    : '\033[36m',  # Cyan
              'INFO'     : '\033[32m',  # Green
              'WARNING'  : '\033[33m',  # Yellow
              'ERROR'    : '\033[31m',  # Red
              'CRITICAL' : '\033[35m',  # Magenta
              'RESET'    : '\033[0m',
             }
    

    def format(self, record):
        if sys.stdout.isatty():
            levelname = record.levelname

            if (levelname in self.COLORS):
                record.levelname = f"{self.COLORS[levelname]}{levelname}{self.COLORS['RESET']}"
        
        return super().format(record)


def setup_logger(name: str = None) -> logging.Logger:
    """
    Setup logger with console and file handlers
    
    Arguments:
    ----------
        name   { str }     : Logger name (defaults to root logger)
    
    Returns:
    --------
        { logging.Logger } : Configured logger instance
    """
    logger = logging.getLogger(name or settings.APP_NAME)
    
    # Avoid duplicate handlers
    if logger.handlers:
        return logger
    
    level             = getattr(logging, settings.LOG_LEVEL, logging.INFO)
    logger.setLevel(level)

    logger.propagate  = False
    
    # Console handler with colors
    console_handler   = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.DEBUG if settings.DEBUG else logging.INFO)
    
    console_formatter = ColoredFormatter('%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',
                                         datefmt = '%Y-%m-%d %H:%M:%S'
                                        )
    console_handler.setFormatter(console_formatter)

    logger.addHandler(console_handler)
    
    # File handler
    log_file          = settings.LOGS_DIR / f"app_{datetime.now().strftime('%Y%m%d')}.log"
    file_handler      = logging.FileHandler(log_file)
    file_handler.setLevel(logging.DEBUG)

    file_formatter    = logging.Formatter('%(asctime)s | %(levelname)-8s | %(name)s | %(funcName)s:%(lineno)d | %(message)s',
                                          datefmt = '%Y-%m-%d %H:%M:%S'
                                         )

    file_handler.setFormatter(file_formatter)
    
    logger.addHandler(file_handler)
    
    return logger


def get_logger(name: str = None) -> logging.Logger:
    """
    Get or create logger instance
    """
    return setup_logger(name)

######################
# utils/helpers.py
######################
# Dependencies
import re
import uuid
import hashlib
from pathlib import Path
from datetime import datetime
from datetime import timedelta
from utils.logger import get_logger


# Setup Logging
logger = get_logger(__name__)


def generate_unique_id() -> str:
    """
    Generate unique ID for files/reports
    """
    unique_id = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
    
    return unique_id


def calculate_hash(file_path: Path) -> str:
    """
    Calculate SHA256 hash of file
    """
    sha256 = hashlib.sha256()

    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            sha256.update(chunk)
    
    hash = sha256.hexdigest()

    return hash


def format_filesize(size_bytes: int) -> str:
    """
    Format file size in human-readable format
    """
    for unit in ['B', 'KB', 'MB', 'GB']:
        if (size_bytes < 1024.0):
            return f"{size_bytes:.2f} {unit}"

        size_bytes /= 1024.0

    file_size = f"{size_bytes:.2f} TB"
    
    return file_size


def cleanup_old_files(directory: Path, days: int = 7) -> int:
    """
    Clean up files older than specified days
    
    Arguments:
    ----------
        directory { Path } : Directory to clean
        
        days      { int }  : Files older than this will be deleted
    
    Returns:
    --------
            { int }        : Number of files deleted
    """
    if not directory.exists():
        return 0
    
    cutoff  = datetime.now() - timedelta(days = days)
    deleted = 0
    
    for file_path in directory.iterdir():
        if file_path.is_file():
            file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            
            if (file_time < cutoff):
                try:
                    file_path.unlink()
                    deleted += 1
                    logger.debug(f"Deleted old file: {file_path.name}")

                except Exception as e:
                    logger.error(f"Failed to delete {file_path.name}: {e}")
    
    if (deleted > 0):
        logger.info(f"Cleaned up {deleted} files from {directory.name}")
    
    return deleted


def safe_filename(filename: str) -> str:
    """
    Sanitize filename for safe storage
    """
    # Remove any path components
    filename = Path(filename).name
    
    # Replace unsafe characters
    filename = re.sub(r'[^\w\s.-]', '', filename)
    
    # Limit length
    if (len(filename) > 255):
        name, ext = filename.rsplit('.', 1) if '.' in filename else (filename, '')
        filename  = name[:250] + ('.' + ext if ext else '')
        
    return filename

######################
# utils/validators.py
######################
# Dependencies
import magic
from PIL import Image
from pathlib import Path
from typing import Tuple
from utils.logger import get_logger
from config.settings import settings
from config.constants import MIN_IMAGE_DIMENSION
from config.constants import MAX_IMAGE_DIMENSION


# Setup Logging
logger = get_logger(__name__)


class ValidationError(Exception):
    """
    Custom validation error
    """
    pass


class ImageValidator:
    """
    Validate uploaded images
    """
    @staticmethod
    def validate_file_size(file_size: int) -> None:
        """
        Validate file size
        """
        if (file_size > settings.max_file_size_bytes):
            raise ValidationError(f"File size {file_size} bytes exceeds maximum {settings.max_file_size_bytes} bytes")

        if (file_size == 0):
            raise ValidationError("File is empty")
    

    @staticmethod
    def validate_file_extension(filename: str) -> None:
        """
        Validate file extension
        """
        extension = Path(filename).suffix.lower()
        
        if (extension not in settings.ALLOWED_EXTENSIONS):
            raise ValidationError(f"File extension {extension} not allowed. Allowed: {', '.join(settings.ALLOWED_EXTENSIONS)}")
    

    @staticmethod
    def validate_image_content(file_path: Path) -> Tuple[int, int]:
        """
        Validate image can be opened and get dimensions
        """
        try:
            with Image.open(file_path) as image:
                width, height = image.size
                
                # Validate dimensions
                if ((width < MIN_IMAGE_DIMENSION) or (height < MIN_IMAGE_DIMENSION)):
                    raise ValidationError(f"Image dimensions ({width}x{height}) too small. Minimum: {MIN_IMAGE_DIMENSION}px")
                
                if ((width > MAX_IMAGE_DIMENSION) or (height > MAX_IMAGE_DIMENSION)):
                    raise ValidationError(f"Image dimensions ({width}x{height}) too large. Maximum: {MAX_IMAGE_DIMENSION}px")
                
                # Verify format
                if (image.format.lower() not in ['jpeg', 'png', 'webp']):
                    raise ValidationError(f"Unsupported image format: {image.format}")
                
                return width, height
                
        except ValidationError:
            raise

        except Exception as e:
            raise ValidationError(f"Cannot open image: {str(e)}")
    

    @staticmethod
    def validate_mime_type(file_path: Path) -> None:
        """
        Validate MIME type matches image
        """
        try:
            mime = magic.from_file(str(file_path), mime = True)

            if (not mime.startswith('image/')):
                raise ValidationError(f"File is not an image. MIME type: {mime}")
        
        except Exception as e:
            logger.warning(f"MIME type validation failed: {e}")
            # Don't fail if python-magic is not available
    

    @classmethod
    def validate_image(cls, file_path: Path, filename: str, file_size: int) -> Tuple[int, int]:
        """
        Comprehensive image validation
        """
        cls.validate_file_size(file_size)
        cls.validate_file_extension(filename)

        dimensions = cls.validate_image_content(file_path)
        cls.validate_mime_type(file_path)  # Optional, commented out if python-magic not available
        
        logger.debug(f"Validated image: {filename} ({dimensions[0]}x{dimensions[1]})")

        return dimensions

############################
# utils/image_processor.py
############################
# Dependencies
import cv2
import numpy as np
from PIL import Image
from pathlib import Path
from typing import Tuple
from typing import Optional
from utils.logger import get_logger
from config.constants import LUMINANCE_WEIGHTS


# Setup Logging
logger = get_logger(__name__)


class ImageProcessor:
    """
    Image loading and preprocessing utilities
    """
    @staticmethod
    def load_image(file_path: Path) -> np.ndarray:
        """
        Load image as numpy array in RGB format
        
        Arguments:
        ----------
            file_path { Path } : Path of the image file needs to be loaded

        Returns:
        --------
            { np.ndarray }     : Image array in RGB format (H, W, 3)
        """
        try:
            image = cv2.imread(str(file_path))
            
            if image is None:
                raise ValueError(f"Failed to load image: {file_path}")
            
            # Convert BGR to RGB
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            logger.debug(f"Loaded image: {file_path.name} shape={image.shape}")
            return image

        except Exception as e:
            logger.error(f"Error loading image {file_path}: {e}")
            raise

    
    @staticmethod
    def rgb_to_luminance(image: np.ndarray) -> np.ndarray:
        """
        Convert RGB image to luminance using ITU-R BT.709 standard
        
        Arguments:
        ----------
            image { np.ndarray } : RGB image array (H, W, 3)
        
        Returns:
        --------
             { np.ndarray }      : Luminance array (H, W)
        """
        if ((image.ndim != 3) or (image.shape[2] != 3)):
            raise ValueError(f"Expected RGB image (H, W, 3), got shape {image.shape}")
        
        r, g, b   = LUMINANCE_WEIGHTS
        
        luminance = r * image[:, :, 0] + g * image[:, :, 1] + b * image[:, :, 2]
        
        return luminance.astype(np.float32)

    
    @staticmethod
    def compute_gradients(luminance: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Compute Sobel gradients
        
        Arguments:
        ----------
            luminance { np.ndarray } : Luminance array (H, W)
        
        Returns:
        --------
                   { tuple }         : Tuple of (gradient_x, gradient_y)
        """
        gx = cv2.Sobel(luminance, cv2.CV_64F, 1, 0, ksize = 3)
        gy = cv2.Sobel(luminance, cv2.CV_64F, 0, 1, ksize = 3)

        return gx, gy
    

    @staticmethod
    def normalize_image(image: np.ndarray) -> np.ndarray:
        """
        Normalize image to [0, 1] range
        """
        normalized_image = image.astype(np.float32) / 255.0
        
        return normalized_image
    

    @staticmethod
    def resize_if_needed(image: np.ndarray, max_dimension: int = 2048) -> np.ndarray:
        """
        Resize image if larger than max_dimension while maintaining aspect ratio
        
        Arguments:
        ----------
            image        { np.ndarray } : Input image

            max_dimension   { int }     : Maximum dimension (width or height)
        
        Returns:
        --------
                 { np.ndarray }         : Resized image if needed, otherwise original
        """
        h, w = image.shape[:2]

        if (max(h, w) <= max_dimension):
            return image
        
        scale   = max_dimension / max(h, w)
        new_w   = int(w * scale)
        new_h   = int(h * scale)
        
        resized = cv2.resize(image, (new_w, new_h), interpolation = cv2.INTER_AREA)
        
        logger.debug(f"Resized image from {w}x{h} to {new_w}x{new_h}")

        return resized
    
    @staticmethod
    def extract_patches(image: np.ndarray, patch_size: int, stride: int, max_patches: Optional[int] = None) -> np.ndarray:
        """
        Extract patches from image
        
        Arguments:
        ----------
            image      { np.ndarray } : Input image (H, W) or (H, W, C)
            
            patch_size    { int }     : Size of patches
            
            stride        { int }     : Stride between patches
            
            max_patches   { int }     : Maximum number of patches to extract
        
        Returns:
        --------
                 { np.ndarray }       : Array of patches
        """
        h, w    = image.shape[:2]
        patches = list()
        
        for y in range(0, h - patch_size + 1, stride):
            for x in range(0, w - patch_size + 1, stride):
                patch = image[y:y+patch_size, x:x+patch_size]

                patches.append(patch)
                
                if (max_patches and (len(patches) >= max_patches)):
                    return np.array(patches)
        
        return np.array(patches)


#################################
# metrics/gradient_field_pca.py
#################################

# Dependencies
import numpy as np
from utils.logger import get_logger
from config.schemas import MetricResult
from config.constants import MetricType
from utils.image_processor import ImageProcessor
from config.constants import GRADIENT_FIELD_PCA_PARAMS


# Setup Logging
logger = get_logger(__name__)


class GradientFieldPCADetector:
    """
    Detects AI-generated images by analyzing gradient field consistency. Real photos have consistent gradient 
    patterns shaped by physics (lighting, optics). Diffusion models struggle to maintain physically consistent
    gradients due to denoising
    
    Core principle:
    ---------------
    - Real photos : Gradients align with physical light sources (low-dimensional structure)
    - AI images   : Gradients are inconsistent due to patch-based denoising (high-dimensional)
    
    Method:
    -------
    1. Convert to luminance
    2. Compute Sobel gradients (Gx, Gy)
    3. Flatten to gradient vectors per pixel
    4. Compute covariance matrix
    5. PCA eigenvalue analysis
    """
    def __init__(self):
        """
        Initialize Gradient-Field PCA Detector class
        """
        self._range          = np.random.default_rng(seed = GRADIENT_FIELD_PCA_PARAMS.RANDOM_SEED)
        self.image_processor = ImageProcessor()
    

    def detect(self, image: np.ndarray) -> MetricResult:
        """
        Run gradient PCA detection
        
        Arguments:
        ----------
            image { np.ndarray } : RGB image array (H, W, 3)
        
        Returns:
        --------
            { MetricResult }     : Structured metric result containing:
                                   - score      : Suspicion score [0.0, 1.0] (0 = natural, 1 = suspicious)
                                   - confidence : Confidence of this metric's assessment [0.0, 1.0]
                                   - details    : Explainability metadata for UI and reports
        """
        try:
            logger.debug(f"Running gradient PCA detection on image shape {image.shape}")
            
            # Convert image to luminance
            luminance             = self.image_processor.rgb_to_luminance(image = image)
            
            # Compute gradients
            gx, gy                = self.image_processor.compute_gradients(luminance = luminance)
            
            # Flatten and sample gradient vectors
            gradient_vectors      = self._prepare_and_sample_gradients(gx = gx, 
                                                                       gy = gy,
                                                                      )
            
            # Perform PCA
            eigenvalue_ratio      = self._compute_eigenvalue_ratio(gradient_vectors = gradient_vectors)

            if ((len(gradient_vectors) < GRADIENT_FIELD_PCA_PARAMS.MIN_SAMPLES) or (eigenvalue_ratio == GRADIENT_FIELD_PCA_PARAMS.NEUTRAL_SCORE)):
                return MetricResult(metric_type = MetricType.GRADIENT,
                                    score       = GRADIENT_FIELD_PCA_PARAMS.NEUTRAL_SCORE,
                                    confidence  = 0.0,
                                    details     = {"reason"           : "insufficient_gradient_information",
                                                   "original_pixels"  : int(gx.size),
                                                   "filtered_vectors" : int(len(gradient_vectors)),
                                                  },
                                   )
            
            # Convert to suspicion score
            suspicion_score       = self._eigenvalue_to_suspicion(eigenvalue_ratio = eigenvalue_ratio)

            # Confidence inverted relative to suspicion: High eigenvalue_ratio = natural, High suspicion_score = AI-like
            confidence            = abs(eigenvalue_ratio - GRADIENT_FIELD_PCA_PARAMS.EIGENVALUE_RATIO_THRESHOLD)
            normalized_confidence = np.clip((confidence / GRADIENT_FIELD_PCA_PARAMS.EIGENVALUE_RATIO_THRESHOLD), 0.0, 1.0)
            
            logger.debug(f"Gradient PCA: eigenvalue_ratio={eigenvalue_ratio:.3f}, suspicion_score={suspicion_score:.3f}")
            
            return MetricResult(metric_type = MetricType.GRADIENT,
                                score       = float(suspicion_score),
                                confidence  = float(normalized_confidence),
                                details     = {"gradient_vectors_sampled" : len(gradient_vectors),
                                               "eigenvalue_ratio"         : float(eigenvalue_ratio),
                                               "threshold"                : GRADIENT_FIELD_PCA_PARAMS.EIGENVALUE_RATIO_THRESHOLD,
                                               "original_pixels"          : int(gx.size),
                                               "filtered_vectors"         : int(len(gradient_vectors)),
                                              },
                               )
            
        except Exception as e:
            logger.error(f"Gradient PCA detection failed: {e}")
            
            # Return neutral score on error
            return MetricResult(metric_type = MetricType.GRADIENT,
                                score       = GRADIENT_FIELD_PCA_PARAMS.NEUTRAL_SCORE,
                                confidence  = 0.0,
                                details     = {"error" : "Gradient PCA detection failed"},
                               )
    

    def _prepare_and_sample_gradients(self, gx: np.ndarray, gy: np.ndarray) -> np.ndarray:
        """
        Flatten gradients into vectors and sample
        
        Arguments:
        ----------
            gx { np.ndarray } : Gradient in x direction

            gy { np.ndarray } : Gradient in y direction
        
        Returns:
        --------
            { np.ndarray }    : Array of gradient vectors (N, 2) where N <= SAMPLE_SIZE
        """
        # Flatten to vectors
        gx_flat                   = gx.flatten()
        gy_flat                   = gy.flatten()

        # Stack into (N, 2) array
        gradient_vectors          = np.stack([gx_flat, gy_flat], axis = 1)
        original_n                = len(gradient_vectors)

        # Remove zero gradients (uniform regions)
        magnitude                 = np.linalg.norm(gradient_vectors, axis = 1)
        non_zero_mask             = (magnitude > GRADIENT_FIELD_PCA_PARAMS.MAGNITUDE_THRESHOLD)
        finite_mask               = np.isfinite(gradient_vectors).all(axis = 1)

        # Filtering Gradient Vector
        filtered_gradient_vectors = gradient_vectors[non_zero_mask & finite_mask]
        filtered_n                = len(filtered_gradient_vectors)
        
        # Sample if too many points without replacement
        if (len(filtered_gradient_vectors) > GRADIENT_FIELD_PCA_PARAMS.SAMPLE_SIZE):
            indices                   = self._range.choice(a       = len(filtered_gradient_vectors), 
                                                           size    = GRADIENT_FIELD_PCA_PARAMS.SAMPLE_SIZE, 
                                                           replace = False,
                                                          )

            sampled_gradient_vectors  = filtered_gradient_vectors[indices]
        
        else:
            sampled_gradient_vectors  = filtered_gradient_vectors


        sampled_n = len(sampled_gradient_vectors)

        logger.debug(f"Gradient PCA sampling: original={original_n}, filtered={filtered_n}, sampled={sampled_n}")
        
        return sampled_gradient_vectors
    

    def _compute_eigenvalue_ratio(self, gradient_vectors: np.ndarray) -> float:
        """
        Compute ratio of first eigenvalue to total variance
        
        -  Lower ratio  = more diffuse structure = suspicious
        -  Higher ratio = concentrated structure = natural
        
        Arguments:
        ----------
            gradient_vectors { np.ndarray } : Array of gradient vectors (N, 2)
        
        Returns:
        --------
                     { float }              : Ratio of first eigenvalue to sum of eigenvalues
        """
        if (len(gradient_vectors) < GRADIENT_FIELD_PCA_PARAMS.MIN_SAMPLES):
            logger.warning("Insufficient gradient samples for PCA")
            return GRADIENT_FIELD_PCA_PARAMS.NEUTRAL_SCORE
        
        # Compute covariance matrix
        covariance       = np.cov(m    = gradient_vectors.T,
                                  bias = True,
                                 )
        
        # Compute eigenvalues
        eigenvalues      = np.linalg.eigvalsh(covariance)

        # Sort in descending order
        eigenvalues      = np.sort(eigenvalues)[::-1]  
        
        # Ratio of largest eigenvalue to sum
        total_variance   = np.sum(eigenvalues)
        
        if (total_variance < GRADIENT_FIELD_PCA_PARAMS.VARIANCE_THRESHOLD):
            return GRADIENT_FIELD_PCA_PARAMS.NEUTRAL_SCORE
        
        eigenvalue_ratio = eigenvalues[0] / total_variance

        return float(eigenvalue_ratio)
    

    def _eigenvalue_to_suspicion(self, eigenvalue_ratio: float) -> float:
        """
        Convert eigenvalue ratio to suspicion score
        
        - Real photos : High ratio (0.85-0.95) -> Low suspicion
        - AI images   : Low ratio (0.50-0.75) -> High suspicion
        
        Arguments:
        ----------
            eigenvalue_ratio { float } : PCA eigenvalue ratio
        
        Returns:
        --------
                    { float }          : Suspicion score [0.0, 1.0]
        """
        # Invert and scale: higher ratio = lower suspicion
        # Real photos typically have ratio > 0.85 & AI images typically have ratio < 0.75
        if (eigenvalue_ratio >= GRADIENT_FIELD_PCA_PARAMS.EIGENVALUE_RATIO_THRESHOLD):
            # Strong gradient alignment = likely real
            suspicion = max(0.0, (1.0 - eigenvalue_ratio) * 2.0)

        else:
            # Weak alignment = suspicious
            suspicion = 1.0 - (eigenvalue_ratio / GRADIENT_FIELD_PCA_PARAMS.EIGENVALUE_RATIO_THRESHOLD)
        
        return float(np.clip(suspicion, 0.0, 1.0))

#################################
# metrics/frequency_analyzer.py
#################################
# Dependencies
import numpy as np
from scipy import fft
from utils.logger import get_logger
from config.schemas import MetricResult
from config.constants import MetricType
from utils.image_processor import ImageProcessor
from config.constants import FREQUENCY_ANALYSIS_PARAMS


# Setup Logging
logger = get_logger(__name__)


class FrequencyAnalyzer:
    """
    FFT-based frequency domain analysis for AI detection
    
    Core principle:
    ---------------
    - Real photos : Smooth frequency falloff (natural optical blur)
    - AI images   : Unnatural frequency spikes or gaps (artifacts from generation)
    
    Method:
    -------
    1. Convert to luminance
    2. Compute 2D FFT
    3. Compute radial frequency spectrum
    4. Analyze high-frequency content and distribution patterns
    """
    def __init__(self):
        self.image_processor = ImageProcessor()
    

    def detect(self, image: np.ndarray) -> MetricResult:
        """
        Run frequency domain analysis
        
        Arguments:
        ----------
            image { np.ndarray } : RGB image array (H, W, 3)
        
        Returns:
        --------
            { MetricResult }     : Structured frequency-domain metric result containing:
                                   - score      : Suspicion score [0.0, 1.0]
                                   - confidence : Reliability of frequency evidence
                                   - details    : FFT and spectrum diagnostics
        """
        try:
            logger.debug(f"Running frequency analysis on image shape {image.shape}")
            
            # Convert to luminance
            luminance                   = self.image_processor.rgb_to_luminance(image = image)

            # Normalize luminance (remove DC component for FFT stability)
            normalized_luminance        = luminance - np.mean(luminance)

            if not np.any(normalized_luminance):
                logger.debug("FFT skipped: zero-variance luminance")
                
                return MetricResult(metric_type = MetricType.FREQUENCY,
                                    score       = FREQUENCY_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                                    confidence  = 0.0,
                                    details     = {"reason": "zero_variance_luminance"}
                                   )
            
            # Compute FFT on normalized_luminance
            fft_magnitude               = self._compute_fft_magnitude(luminance = normalized_luminance)
            
            # Analyze radial frequency spectrum
            radial_spectrum             = self._compute_radial_spectrum(fft_magnitude = fft_magnitude)
            
            # Detect anomalies
            anomaly_score, freq_details = self._analyze_frequency_anomalies(radial_spectrum = radial_spectrum)
            
            logger.debug(f"Frequency analysis: Anomaly Score={anomaly_score:.3f}")
            
            # Distance from neutral = stronger evidence = higher confidence
            confidence                  = float(np.clip((abs(anomaly_score - FREQUENCY_ANALYSIS_PARAMS.NEUTRAL_SCORE) * 2.0), 0.0, 1.0))
            
            return MetricResult(metric_type = MetricType.FREQUENCY,
                                score       = float(anomaly_score),
                                confidence  = confidence,
                                details     = {"spectrum_bins" : int(len(radial_spectrum)),
                                                **freq_details,
                                              }
                               )
            
        except Exception as e:
            logger.error(f"Frequency analysis failed: {e}")

            # Return neutral score on error
            return MetricResult(metric_type = MetricType.FREQUENCY,
                                score       = FREQUENCY_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                                confidence  = 0.0,
                                details     = {"error" : "frequency_analysis_failed"},
                               )
    

    def _compute_fft_magnitude(self, luminance: np.ndarray) -> np.ndarray:
        """
        Compute 2D FFT magnitude spectrum
        
        Arguments:
        ----------
            luminance { np.ndarray } : Luminance channel (H, W)
        
        Returns:
        --------
            { np.ndarray }           : FFT magnitude spectrum (centered)
        """
        # Compute 2D FFT
        f             = fft.fft2(luminance)
        
        # Shift zero frequency to center
        f_shifted     = fft.fftshift(f)
        
        # Compute magnitude spectrum
        magnitude     = np.abs(f_shifted)
        
        # Log scale for better visualization
        magnitude_log = np.log1p(magnitude)
        
        return magnitude_log
    

    def _compute_radial_spectrum(self, fft_magnitude: np.ndarray) -> np.ndarray:
        """
        Compute radial average of frequency spectrum
        
        Arguments:
        ----------
            fft_magnitude { np.ndarray } : FFT magnitude spectrum
        
        Returns:
        --------
            { np.ndarray }               : Radial spectrum (1D array)
        """
        h, w                       = fft_magnitude.shape
        center_y, center_x         = h // 2, w // 2
        
        # Create coordinate grids
        y, x                       = np.ogrid[:h, :w]
        
        # Compute radial distances from center
        r                          = np.sqrt((x - center_x)**2 + (y - center_y)**2).astype(int)
        
        # Maximum radius
        max_radius                 = min(center_x, center_y)
        
        # Compute radial bins
        bins                       = np.linspace(0, max_radius, FREQUENCY_ANALYSIS_PARAMS.BINS + 1)
        radial_spectrum            = np.zeros(FREQUENCY_ANALYSIS_PARAMS.BINS)
        
        # Average magnitude in each radial bin
        for i in range(FREQUENCY_ANALYSIS_PARAMS.BINS):
            mask = (r >= bins[i]) & (r < bins[i + 1])
            
            if np.any(mask):
                radial_spectrum[i] = np.mean(fft_magnitude[mask])
        
        return radial_spectrum
    

    def _analyze_frequency_anomalies(self, radial_spectrum: np.ndarray) -> tuple[float, dict]:
        """
        Analyze frequency spectrum for AI generation artifacts
        
        Checks:
        -------
        1. High-frequency content (AI images often have unnatural HF energy)
        2. Frequency distribution smoothness
        3. Spectral slope deviation from natural images
        
        Arguments:
        ----------
            radial_spectrum { np.ndarray } : Radial frequency spectrum
        
        Returns:
        --------
                { tuple }                  : A tuple containing
                                             - Suspicion score [0.0, 1.0], and
                                             - frequency details in a dictionary
        """
        if (len(radial_spectrum) < FREQUENCY_ANALYSIS_PARAMS.MIN_SPECTRUM_SAMPLES):
            return (FREQUENCY_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                    {"reason"        : "insufficient_frequency_samples",
                     "spectrum_bins" : int(len(radial_spectrum)),
                    }
                   )
        
        # Normalize spectrum
        spectrum_norm    = radial_spectrum / (np.max(radial_spectrum) + 1e-10)
        
        # High-frequency Energy Analysis
        high_freq_start  = int(len(spectrum_norm) * FREQUENCY_ANALYSIS_PARAMS.HIGH_FREQ_THRESHOLD)

        if (high_freq_start >= len(spectrum_norm) - 1):
            return (FREQUENCY_ANALYSIS_PARAMS.NEUTRAL_SCORE, 
                    {"reason" : "invalid_frequency_partition"}
                   )

        high_freq_energy = np.mean(spectrum_norm[high_freq_start:])
        low_freq_energy  = np.mean(spectrum_norm[:high_freq_start])
        
        hf_ratio         = high_freq_energy / (low_freq_energy + 1e-10)
        
        # Natural images : HF ratio typically 0.1-0.3
        # AI images      : Can be higher (0.3-0.6) or lower (<0.1)
        hf_anomaly       = 0.0
        
        if (hf_ratio > FREQUENCY_ANALYSIS_PARAMS.HF_RATIO_UPPER):
            hf_anomaly  = min(1.0, (hf_ratio - FREQUENCY_ANALYSIS_PARAMS.HF_RATIO_UPPER) * FREQUENCY_ANALYSIS_PARAMS.HF_UPPER_SCALE)
        
        elif (hf_ratio < FREQUENCY_ANALYSIS_PARAMS.HF_RATIO_LOWER):
            hf_anomaly  = min(1.0, (FREQUENCY_ANALYSIS_PARAMS.HF_RATIO_LOWER - hf_ratio) * FREQUENCY_ANALYSIS_PARAMS.HF_LOWER_SCALE)
        
        # Spectral Smoothness Analysis
        spectral_diff   = np.abs(np.diff(spectrum_norm))
        roughness       = np.mean(spectral_diff)
        roughness_score = np.clip(roughness * FREQUENCY_ANALYSIS_PARAMS.ROUGHNESS_SCALE, 0.0, 1.0)
        
        # Power Law Deviation Analysis
        x               = np.arange(1, len(spectrum_norm) + 1)
        log_spectrum    = np.log(spectrum_norm + 1e-10)
        log_x           = np.log(x)
        
        # Linear fit in log-log space
        coeffs          = np.polyfit(log_x, log_spectrum, 1)
        fitted          = np.polyval(coeffs, log_x)
        deviation       = np.mean(np.abs(log_spectrum - fitted))
        deviation_score = np.clip(deviation * FREQUENCY_ANALYSIS_PARAMS.DEVIATION_SCALE, 0.0, 1.0)
        
        # Combine scores
        weights         = FREQUENCY_ANALYSIS_PARAMS.SUBMETRIC_WEIGHTS

        combined_score  = (weights['hf_anomaly'] * hf_anomaly + weights['roughness'] * roughness_score + weights['deviation'] * deviation_score)

        final_score     = float(np.clip(combined_score, 0.0, 1.0))
        
        frequency_dict  = {"low_freq_energy"     : float(low_freq_energy),
                           "high_freq_energy"    : float(high_freq_energy),
                           "hf_ratio"            : float(hf_ratio),
                           "hf_anomaly"          : float(hf_anomaly),
                           "roughness"           : float(roughness),
                           "roughness_score"     : float(roughness_score),
                           "spectral_deviation"  : float(deviation),
                           "deviation_score"     : float(deviation_score),
                           "high_freq_start_bin" : int(high_freq_start),
                          }

        logger.debug(f"FFT scores - HF anomaly: {hf_anomaly:.3f}, roughness: {roughness_score:.3f}, deviation: {deviation_score:.3f}")
        
        return (final_score, frequency_dict)

#################################
# metrics/noise_analyzer.py
#################################
# Dependencies
import numpy as np
from utils.logger import get_logger
from config.schemas import MetricResult
from config.constants import MetricType
from utils.image_processor import ImageProcessor
from config.constants import NOISE_ANALYSIS_PARAMS


# Setup Logging
logger = get_logger(__name__)


class NoiseAnalyzer:
    """
    Noise pattern analysis for AI detection
    
    Core principle:
    ---------------
    - Real photos : Sensor noise follows Poisson distribution (shot noise) + Gaussian (read noise)
    - AI images   : Too uniform, artificially smooth, or completely missing noise
    
    Method:
    -------
    1. Extract local patches
    2. Estimate noise variance in each patch
    3. Analyze noise consistency and distribution
    4. Check for unnatural uniformity
    """
    def __init__(self):
        self.image_processor = ImageProcessor()
    

    def detect(self, image: np.ndarray) -> MetricResult:
        """
        Run noise pattern analysis
        
        Arguments:
        ----------
            image { np.ndarray } : RGB image array (H, W, 3)
        
        Returns:
        --------
            { MetricResult }     : Structured Noise-domain metric result containing:
                                   - score      : Suspicion score [0.0, 1.0]
                                   - confidence : Reliability of noise evidence
                                   - details    : Noise related diagnostics
        """
        try:
            logger.debug(f"Running noise analysis on image shape {image.shape}")
            
            # Convert to luminance
            luminance                  = self.image_processor.rgb_to_luminance(image = image)
            
            # Extract patches
            patches                    = self._extract_patches(luminance = luminance)
            
            if (len(patches) == 0):
                logger.warning("No patches extracted for noise analysis")
                return MetricResult(metric_type = MetricType.NOISE,
                                    score       = NOISE_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                                    confidence  = 0.0,
                                    details     = {"reason": "no_patches_extracted"},
                                   )
            
            # Estimate noise in each patch
            noise_estimates, mad_values, laplacian_energy = self._estimate_noise_per_patch(patches = patches)
            
            # Filter Noise Estimates, MAD and Laplacian Energy for finite values only
            filtered_mask                                 = np.isfinite(noise_estimates)
            filtered_noise_estimates                      = noise_estimates[filtered_mask]
            filtered_mad                                  = mad_values[filtered_mask]
            filtered_laplacian_energy                     = laplacian_energy[filtered_mask]
            
            if (len(filtered_noise_estimates) < NOISE_ANALYSIS_PARAMS.MIN_ESTIMATES):
                logger.debug("Insufficient valid noise estimates after filtering")

                return MetricResult(metric_type = MetricType.NOISE,
                                    score       = NOISE_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                                    confidence  = 0.0,
                                    details     = {"reason"        : "insufficient_noise_estimates",
                                                   "patches_total" : int(len(patches)),
                                                   "patches_valid" : int(len(filtered_noise_estimates)),
                                                  },
                                   )

            logger.debug(f"Noise patches: total={len(patches)}, valid={len(filtered_noise_estimates)}")
            
            # Analyze noise distribution
            noise_score, noise_details                    = self._analyze_noise_distribution(noise_estimates  = filtered_noise_estimates,
                                                                                             mad_values       = filtered_mad,
                                                                                             laplacian_energy = filtered_laplacian_energy,
                                                                                            )
            
            # Confidence: distance from neutral
            confidence                                    = float(np.clip((abs(noise_score - NOISE_ANALYSIS_PARAMS.NEUTRAL_SCORE) * 2.0), 0.0, 1.0))
            
            logger.debug(f"Noise analysis: score={noise_score:.3f}, patches={len(patches)}, valid={len(filtered_noise_estimates)}")
            
            return MetricResult(metric_type = MetricType.NOISE,
                                score       = float(noise_score),
                                confidence  = confidence,
                                details     = {"patches_total" : int(len(patches)),
                                               "patches_valid" : int(len(filtered_noise_estimates)),
                                               **noise_details,
                                              },
                               )
            
        except Exception as e:
            logger.error(f"Noise analysis failed: {e}")
            
            # Return neutral score on error
            return MetricResult(metric_type = MetricType.NOISE,
                                score       = NOISE_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                                confidence  = 0.0,
                                details     = {"error": "noise_analysis_failed"},
                               )
                        

    def _extract_patches(self, luminance: np.ndarray) -> np.ndarray:
        """
        Extract patches from image for local noise estimation
        
        Arguments:
        ----------
            luminance { np.ndarray } : Luminance channel (H, W)
        
        Returns:
        --------
            { np.ndarray }           : Array of patches
        """
        patches = self.image_processor.extract_patches(image       = luminance,
                                                       patch_size  = NOISE_ANALYSIS_PARAMS.PATCH_SIZE,
                                                       stride      = NOISE_ANALYSIS_PARAMS.STRIDE,
                                                       max_patches = NOISE_ANALYSIS_PARAMS.SAMPLES,
                                                      )
        
        return patches
    

    def _estimate_noise_per_patch(self, patches: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Estimate noise variance in each patch using median absolute deviation
        
        Uses Median Absolute Deviation (MAD) which is robust to edges/textures
        
        Arguments:
        ----------
            patches { np.ndarray } : Array of image patches (N, patch_size, patch_size)
        
        Returns:
        --------
                 { tuple }         : A tuple containing
                                     - Array of noise estimates per patch
                                     - Array of MAD values
                                     - Array of Laplacian Energy Values
        """
        noise_estimates          = list()
        mad_values               = list()
        laplacian_energy_values  = list()
        
        for patch in patches:
            # Skip patches with too much structure (edges, textures)
            variance = np.var(patch)
            
            if (variance < NOISE_ANALYSIS_PARAMS.VARIANCE_LOW_THRESHOLD):     
                # Too uniform, skip
                continue
            
            if (variance > NOISE_ANALYSIS_PARAMS.VARIANCE_HIGH_THRESHOLD):  
                # Too much structure, skip
                continue
            
            # Use Median Absolute Deviation for robust noise estimation
            laplacian   = self._apply_laplacian(patch = patch)
            mad         = np.median(np.abs(laplacian - np.median(laplacian)))
            
            # Convert MAD to noise standard deviation estimate: For Gaussian noise: σ ≈ 1.4826 × MAD
            noise_std   = NOISE_ANALYSIS_PARAMS.MAD_TO_STD_FACTOR * mad

            # Calculate Laplacian Energy
            lap_energy  = float(np.mean(laplacian ** 2))
            
            # Append corresponding values to their storages
            mad_values.append(mad)
            noise_estimates.append(noise_std)
            laplacian_energy_values.append(lap_energy)
        
        return np.array(noise_estimates), np.array(mad_values), np.array(laplacian_energy_values)
    

    def _apply_laplacian(self, patch: np.ndarray) -> np.ndarray:
        """
        Apply Laplacian filter to isolate high-frequency noise
        
        Arguments:
        ----------
            patch { np.ndarray } : Image patch
        
        Returns:
        --------
            { np.ndarray }       : Laplacian-filtered patch
        """
        # Simple 3x3 Laplacian kernel
        kernel = np.array([[0,  1, 0],
                          [1, -4, 1],
                          [0,  1, 0]],
                         )
        
        # Pad patch
        padded = np.pad(patch, 1, mode = 'reflect')
        
        # Apply convolution
        h, w   = patch.shape
        result = np.zeros_like(patch)
        
        for i in range(h):
            for j in range(w):
                region        = padded[i:i+3, j:j+3]
                result[i, j]  = np.sum(region * kernel)
        
        return result
    

    def _analyze_noise_distribution(self, noise_estimates: np.ndarray, mad_values: np.ndarray, laplacian_energy: np.ndarray,) -> tuple[float, dict]:
        """
        Analyze noise distribution for anomalies
        
        Checks:
        -------
        1. Coefficient of variation (consistency)
        2. Overall noise level (too low = suspicious)
        3. Distribution shape (too uniform = suspicious)
        
        Arguments:
        ----------
            noise_estimates  { np.ndarray } : Array of noise standard deviations

            mad_values       { np.ndarray } : Array of MAD values

            laplacian_energy { np.ndarray } : Array of Laplacian Energy Values 
        
        Returns:
        --------
                    { tuple }              : A tuple containing:
                                             - Suspicion score [0.0, 1.0]
                                             - Noise Distribution detailed diagnostics
        """
        if (len(noise_estimates) < NOISE_ANALYSIS_PARAMS.MIN_ESTIMATES):
            return (NOISE_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                    {"reason": "insufficient_noise_samples"},
                   )
        
        # Remove outliers (keep middle 80%)
        q10                 = np.percentile(noise_estimates, NOISE_ANALYSIS_PARAMS.OUTLIER_PERCENTILE_LOW)
        q90                 = np.percentile(noise_estimates, NOISE_ANALYSIS_PARAMS.OUTLIER_PERCENTILE_HIGH)
        filtered            = noise_estimates[(noise_estimates >= q10) & (noise_estimates <= q90)]
        
        if (len(filtered) < NOISE_ANALYSIS_PARAMS.MIN_FILTERED_SAMPLES):
            return (NOISE_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                    {"reason": "insufficient_filtered_samples"},
                   ) 
        
        mean_noise          = np.mean(filtered)
        std_noise           = np.std(filtered)
        
        # Coefficient of Variation (CV) Analysis
        cv                  = std_noise / (mean_noise + 1e-10)
        cv_anomaly          = 0.0
        
        if (cv < NOISE_ANALYSIS_PARAMS.CV_UNIFORM_THRESHOLD):
            # Too uniform
            cv_anomaly = (NOISE_ANALYSIS_PARAMS.CV_UNIFORM_THRESHOLD - cv) * NOISE_ANALYSIS_PARAMS.CV_UNIFORM_SCALE
        
        elif (cv > NOISE_ANALYSIS_PARAMS.CV_VARIABLE_THRESHOLD):
            # Too variable
            cv_anomaly = min(1.0, (cv - NOISE_ANALYSIS_PARAMS.CV_VARIABLE_THRESHOLD) * NOISE_ANALYSIS_PARAMS.CV_VARIABLE_SCALE)
        
        # Overall noise level Analysis
        noise_level_anomaly = 0.0

        if (mean_noise < NOISE_ANALYSIS_PARAMS.LEVEL_CLEAN_THRESHOLD):
            # Too clean
            noise_level_anomaly = (NOISE_ANALYSIS_PARAMS.LEVEL_CLEAN_THRESHOLD - mean_noise) / NOISE_ANALYSIS_PARAMS.LEVEL_CLEAN_THRESHOLD
        
        elif (mean_noise < NOISE_ANALYSIS_PARAMS.LEVEL_LOW_THRESHOLD):
            # Slightly low
            noise_level_anomaly = (NOISE_ANALYSIS_PARAMS.LEVEL_LOW_THRESHOLD - mean_noise) / NOISE_ANALYSIS_PARAMS.LEVEL_LOW_THRESHOLD * 0.5

        
        # Distribution shape Analysis
        q25                 = np.percentile(filtered, NOISE_ANALYSIS_PARAMS.IQR_PERCENTILE_LOW)
        q75                 = np.percentile(filtered, NOISE_ANALYSIS_PARAMS.IQR_PERCENTILE_HIGH)
        iqr                 = q75 - q25
        iqr_ratio           = iqr / (mean_noise + 1e-10)
        
        iqr_anomaly         = 0.0
        
        if (iqr_ratio < NOISE_ANALYSIS_PARAMS.IQR_THRESHOLD):
            iqr_anomaly = (NOISE_ANALYSIS_PARAMS.IQR_THRESHOLD - iqr_ratio) * NOISE_ANALYSIS_PARAMS.IQR_SCALE

        # Clip sub-anomalies for safety
        cv_anomaly          = np.clip(cv_anomaly, 0.0, 1.0)
        noise_level_anomaly = np.clip(noise_level_anomaly, 0.0, 1.0)
        iqr_anomaly         = np.clip(iqr_anomaly, 0.0, 1.0)

        # Combine scores
        weights             = NOISE_ANALYSIS_PARAMS.SUBMETRIC_WEIGHTS
        combined_score      = (weights['cv_anomaly'] * cv_anomaly + weights['noise_level_anomaly'] * noise_level_anomaly + weights['iqr_anomaly'] * iqr_anomaly)
        final_score         = float(np.clip(combined_score, 0.0, 1.0))

        # Calculate Forensic Stats
        mad_mean            = float(np.mean(mad_values)) if len(mad_values) else 0.0
        laplacian_energy_mu = float(np.mean(laplacian_energy)) if len(laplacian_energy) else 0.0

        noise_details_dict  = {"mean_noise"          : float(mean_noise),
                               "std_noise"           : float(std_noise),
                               "cv"                  : float(cv),
                               "cv_anomaly"          : float(cv_anomaly),
                               "noise_level_anomaly" : float(noise_level_anomaly),
                               "iqr_ratio"           : float(iqr_ratio),
                               "iqr_anomaly"         : float(iqr_anomaly),
                               "mad_mean"            : mad_mean,
                               "laplacian_energy"    : laplacian_energy_mu,
                              }

        logger.debug(f"Noise scores - CV: {cv:.3f}, mean: {mean_noise:.3f}, IQR ratio: {iqr_ratio:.3f}")

        return final_score, noise_details_dict

#################################
# metrics/texture_analyzer.py
#################################
# Dependencies
import numpy as np
from scipy.stats import entropy
from utils.logger import get_logger
from config.schemas import MetricResult
from config.constants import MetricType
from utils.image_processor import ImageProcessor
from config.constants import TEXTURE_ANALYSIS_PARAMS


# Setup Logging
logger = get_logger(__name__)


class TextureAnalyzer:
    """
    Statistical texture analysis for AI detection
    
    Core principle:
    ---------------
    - Real photos : Natural texture variation (random but structured)
    - AI images   : Either too smooth or repetitive patterns
    
    Method:
    -------
    1. Extract local patches
    2. Compute texture features (contrast, entropy)
    3. Analyze texture consistency and distribution
    4. Detect unnaturally smooth regions
    """
    def __init__(self):
        """
        Initialize TextureAnalyzer Class
        """
        self.patch_size      = TEXTURE_ANALYSIS_PARAMS.PATCH_SIZE
        self.n_patches       = TEXTURE_ANALYSIS_PARAMS.N_PATCHES
        self.image_processor = ImageProcessor()
        self._rng            = np.random.default_rng(seed = TEXTURE_ANALYSIS_PARAMS.RANDOM_SEED)
    

    def detect(self, image: np.ndarray) -> MetricResult:
        """
        Run texture analysis
        
        Arguments:
        ----------
            image { np.ndarray } : RGB image array (H, W, 3)
        
        Returns:
        --------
            { MetricResult }     : Structured Texture-domain metric result containing:
                                   - score      : Suspicion score [0.0, 1.0]
                                   - confidence : Reliability of texture evidence
                                   - details    : Texture forensics and statistics
        """
        try:
            logger.debug(f"Running texture analysis on image shape {image.shape}")
            
            # Convert to luminance
            luminance                          = self.image_processor.rgb_to_luminance(image = image)
            
            # Extract patches
            patches                            = self._extract_patches(luminance = luminance)
            
            if (len(patches) == 0):
                logger.warning("No patches extracted for texture analysis")
                return MetricResult(metric_type = MetricType.TEXTURE,
                                    score       = TEXTURE_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                                    confidence  = 0.0,
                                    details     = {"reason": "no_patches_extracted"},
                                   )           
            
            # Compute texture features
            texture_features, texture_metadata = self._compute_texture_features(patches = patches)
            
            # Analyze for anomalies
            texture_score, texture_details     = self._analyze_texture_anomalies(features = texture_features,
                                                                                 metadata = texture_metadata,
                                                                                )
            
            # Calculate Confidence
            confidence                         = float(np.clip((abs(texture_score - TEXTURE_ANALYSIS_PARAMS.NEUTRAL_SCORE) * 2.0), 0.0, 1.0))
            
            logger.debug(f"Texture analysis: Texture Score={texture_score:.3f}, patches={len(patches)}")
            
            return MetricResult(metric_type = MetricType.TEXTURE,
                                score       = float(texture_score),
                                confidence  = confidence,
                                details     = {"patches_total" : int(len(patches)),
                                               **texture_metadata,
                                               **texture_details,
                                              },
                               )
            
        except Exception as e:
            logger.error(f"Texture analysis failed: {e}")
            
            # Return neutral score on error
            return MetricResult(metric_type = MetricType.TEXTURE,
                                score       = TEXTURE_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                                confidence  = 0.0,
                                details     = {"error": "texture_analysis_failed"},
                               )
    

    def _extract_patches(self, luminance: np.ndarray) -> np.ndarray:
        """
        Extract random patches from image
        """
        h, w = luminance.shape
        
        if ((h < self.patch_size) or (w < self.patch_size)):
            logger.warning(f"Image too small for patch size {self.patch_size}")
            return np.array([])
        
        patches = list()
        
        for _ in range(self.n_patches):
            y     = self._rng.integers(0, h - self.patch_size)
            x     = self._rng.integers(0, w - self.patch_size)
            
            patch = luminance[y:y+self.patch_size, x:x+self.patch_size]
            patches.append(patch)
        
        return np.array(patches)
    

    def _compute_texture_features(self, patches: np.ndarray) -> tuple[dict, dict]:
        """
        Compute texture features for each patch
        
        Features:
        ---------
        1. Local contrast (standard deviation)
        2. Entropy (randomness)
        3. Smoothness (inverse of variance)
        4. Edge density
        
        Arguments:
        ----------
            patches { np.ndarray } : Array of patches
        
        Returns:
        --------
            { tuple }              : A tuple containing
                                     - A dictionary of feature arrays
                                     - A dictionary of texture analysis metadata
        """
        contrasts       = list()
        entropies       = list()
        smoothnesses    = list()
        edge_densities  = list()
        uniform_skipped = 0
        
        for patch in patches:
            pmin = patch.min()
            pmax = patch.max()

            if ((pmax - pmin < 1e-6)):
                # skip fully uniform patch entirely
                uniform_skipped += 1
                continue 
             
            # Contrast (std deviation)
            contrast = np.std(patch)
            contrasts.append(contrast)
            
            # Entropy (using histogram)
            hist, _  = np.histogram(patch,
                                    bins  = TEXTURE_ANALYSIS_PARAMS.HISTOGRAM_BINS,
                                    range = TEXTURE_ANALYSIS_PARAMS.HISTOGRAM_RANGE,
                                   )

            hist     = hist / (np.sum(hist) + 1e-10)
            ent      = entropy(hist + 1e-10)
            entropies.append(ent)
            
            # Smoothness (inverse of variance, scaled)
            variance   = np.var(patch)
            smoothness = 1.0 / (1.0 + variance)
            smoothnesses.append(smoothness)
            
            # Edge density (using Sobel)
            gx, gy       = self.image_processor.compute_gradients(luminance = patch)
            gradient_mag = np.sqrt(gx**2 + gy**2)

            edge_density = np.mean(gradient_mag > TEXTURE_ANALYSIS_PARAMS.EDGE_THRESHOLD) 
            edge_densities.append(edge_density)

        # Construct results in proper format
        features = {"contrast"     : np.array(contrasts),
                    "entropy"      : np.array(entropies),
                    "smoothness"   : np.array(smoothnesses),
                    "edge_density" : np.array(edge_densities),
                   }

        metadata = {"patches_used"            : int(len(contrasts)),
                    "uniform_patches_skipped" : int(uniform_skipped),
                   }

        
        return features, metadata
    

    def _analyze_texture_anomalies(self, features: dict, metadata: dict) -> tuple[float, dict]:
        """
        Analyze texture features for AI generation artifacts
        
        Checks:
        -------
        1. Excessive smoothness (too many overly smooth patches)
        2. Entropy distribution (too uniform = suspicious)
        3. Contrast consistency
        
        Arguments:
        ----------
            features { dict } : Dictionary of texture features

            metadata { dict } : Dictionary of texture analysis metadata
        
        Returns:
        --------
            { tuple }         : A tuple containing:
                                - Suspicion score [0.0, 1.0]
                                - Texture statistics
        """
        contrast     = features['contrast']
        entropy_vals = features['entropy']
        smoothness   = features['smoothness']
        edge_density = features['edge_density']

        if ((len(contrast) == 0) or (len(entropy_vals) == 0) or (len(smoothness) == 0) or (len(edge_density) == 0)):
            logger.debug("All texture features filtered out; returning neutral score")
            return (TEXTURE_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                    {"reason": "all_texture_features_filtered"},
                   )

        # Early exit: all patches nearly uniform
        if (np.all(contrast < 1e-6)):
            logger.debug("All texture patches near-uniform; returning neutral score")
            return (TEXTURE_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                    {"reason": "all_patches_near_uniform"},
                   )
        
        # Smoothness Analysis
        smooth_ratio       = np.mean(smoothness > TEXTURE_ANALYSIS_PARAMS.SMOOTHNESS_THRESHOLD)
        smoothness_anomaly = 0.0
        
        if (smooth_ratio > TEXTURE_ANALYSIS_PARAMS.SMOOTH_RATIO_THRESHOLD):
            # More than 40% very smooth patches
            smoothness_anomaly = min(1.0, (smooth_ratio - TEXTURE_ANALYSIS_PARAMS.SMOOTH_RATIO_THRESHOLD) * TEXTURE_ANALYSIS_PARAMS.SMOOTH_RATIO_SCALE)
        
        # Entropy distribution Analysis
        entropy_cv      = np.std(entropy_vals) / (np.mean(entropy_vals) + 1e-10)
        entropy_anomaly = 0.0
        
        if (entropy_cv < TEXTURE_ANALYSIS_PARAMS.ENTROPY_CV_THRESHOLD):  
            # Too uniform
            entropy_anomaly = (TEXTURE_ANALYSIS_PARAMS.ENTROPY_CV_THRESHOLD - entropy_cv) * TEXTURE_ANALYSIS_PARAMS.ENTROPY_SCALE
        
        # Contrast distribution Analysis
        contrast_cv      = np.std(contrast) / (np.mean(contrast) + 1e-10)
        contrast_anomaly = 0.0
        
        if (contrast_cv < TEXTURE_ANALYSIS_PARAMS.CONTRAST_CV_LOW):
            # Too uniform
            contrast_anomaly = (TEXTURE_ANALYSIS_PARAMS.CONTRAST_CV_LOW - contrast_cv) * TEXTURE_ANALYSIS_PARAMS.CONTRAST_LOW_SCALE 
        
        elif (contrast_cv > TEXTURE_ANALYSIS_PARAMS.CONTRAST_CV_HIGH):
            # Too variable (suspicious)
            contrast_anomaly = min(1.0, (contrast_cv - TEXTURE_ANALYSIS_PARAMS.CONTRAST_CV_HIGH) * TEXTURE_ANALYSIS_PARAMS.CONTRAST_HIGH_SCALE)

        
        # Edge density consistency Analysis
        edge_cv      = np.std(edge_density) / (np.mean(edge_density) + 1e-10)
        edge_anomaly = 0.0
        
        if (edge_cv < TEXTURE_ANALYSIS_PARAMS.EDGE_CV_THRESHOLD):
            edge_anomaly = (TEXTURE_ANALYSIS_PARAMS.EDGE_CV_THRESHOLD - edge_cv) * TEXTURE_ANALYSIS_PARAMS.EDGE_SCALE

        # Clipping Sub-anomalies
        smoothness_anomaly = np.clip(smoothness_anomaly, 0.0, 1.0)
        entropy_anomaly    = np.clip(entropy_anomaly, 0.0, 1.0)
        contrast_anomaly   = np.clip(contrast_anomaly, 0.0, 1.0)
        edge_anomaly       = np.clip(edge_anomaly, 0.0, 1.0)
        
        # Combine scores
        weights            = TEXTURE_ANALYSIS_PARAMS.SUBMETRIC_WEIGHTS
        texture_score      = (weights['smoothness_anomaly'] * smoothness_anomaly + weights['entropy_anomaly'] * entropy_anomaly + weights['contrast_anomaly'] * contrast_anomaly + weights['edge_anomaly'] * edge_anomaly)
        final_score        = float(np.clip(texture_score, 0.0, 1.0))

        detailed_stats     = {"smooth_ratio"      : float(smooth_ratio),
                              "entropy_mean"      : float(np.mean(entropy_vals)),
                              "entropy_cv"        : float(entropy_cv),
                              "contrast_mean"     : float(np.mean(contrast)),
                              "contrast_cv"       : float(contrast_cv),
                              "edge_density_mean" : float(np.mean(edge_density)),
                              "edge_cv"           : float(edge_cv),
                             }

        logger.debug(f"Texture scores - smoothness: {smoothness_anomaly:.3f}, entropy: {entropy_anomaly:.3f}, contrast: {contrast_anomaly:.3f}, edge: {edge_anomaly:.3f}")
        
        return final_score, detailed_stats

#################################
# metrics/color_analyzer.py
#################################
# Dependencies
import numpy as np
from utils.logger import get_logger
from config.schemas import MetricResult
from config.constants import MetricType
from utils.image_processor import ImageProcessor
from config.constants import COLOR_ANALYSIS_PARAMS


# Setup Logging
logger = get_logger(__name__)


class ColorAnalyzer:
    """
    Color distribution analysis for AI detection

    Core principle:
    ---------------
    - Real photos : Natural color distributions constrained by physics
    - AI images   : Can create unnatural saturation, hue shifts, or impossible color relationships

    Method:
    -------
    1. Convert to multiple color spaces (RGB, HSV)
    2. Analyze color histogram distributions
    3. Check for oversaturation
    4. Detect unnatural color relationships
    """
    def __init__(self):
        self.image_processor = ImageProcessor()


    def detect(self, image: np.ndarray) -> MetricResult:
        """
        Run color distribution analysis
        
        Arguments:
        ----------
            image { np.ndarray } : RGB image array (H, W, 3)
        
        Returns:
        --------
            { MetricResult }     : Structured Color-domain metric result containing:
                                   - score      : Suspicion score [0.0, 1.0]
                                   - confidence : Reliability of color analysis evidence
                                   - details    : Color Analysis forensics and statistics
        """
        try:
            logger.debug(f"Running color analysis on image shape {image.shape}")
            
            # Normalize image to [0, 1]
            image_norm                           = self.image_processor.normalize_image(image = image)
            
            # Convert to HSV
            hsv                                  = self._rgb_to_hsv(rgb = image_norm)
            
            # Analyze saturation
            saturation_score, saturation_details = self._analyze_saturation(hsv = hsv)
            
            # Analyze color histogram
            histogram_score, histogram_details   = self._analyze_color_histogram(rgb = image_norm)
            
            # Analyze hue distribution
            hue_score, hue_details               = self._analyze_hue_distribution(hsv = hsv)
            
            # Combine scores
            weights                              = COLOR_ANALYSIS_PARAMS.MAIN_WEIGHTS
            final_score                          = (weights['saturation'] * saturation_score + weights['histogram'] * histogram_score + weights['hue'] * hue_score)

            # Calculate Confidence
            confidence                           = float(np.clip((abs(final_score - COLOR_ANALYSIS_PARAMS.NEUTRAL_SCORE) * 2.0), 0.0, 1.0))
            
            logger.debug(f"Color analysis: saturation={saturation_score:.3f}, histogram={histogram_score:.3f}, hue={hue_score:.3f}, Score={final_score:.3f}")
            
            return MetricResult(metric_type = MetricType.COLOR,
                                score       = float(final_score),
                                confidence  = confidence,
                                details     = {"saturation_stats" : saturation_details,
                                               "histogram_stats"  : histogram_details,
                                               "hue_stats"        : hue_details,
                                              },
                               )
            
        except Exception as e:
            logger.error(f"Color analysis failed: {e}")

            # Return neutral score on error
            return MetricResult(metric_type = MetricType.COLOR,
                                score       = COLOR_ANALYSIS_PARAMS.NEUTRAL_SCORE,
                                confidence  = 0.0,
                                details     = {"error": "color_analysis_failed"},
                               )


    def _rgb_to_hsv(self, rgb: np.ndarray) -> np.ndarray:
        """
        Convert RGB to HSV color space
        
        Arguments:
        ----------
            rgb { np.ndarray } : RGB image normalized to [0, 1]
        
        Returns:
        --------
            { np.ndarray }     : HSV image (H in [0, 360], S and V in [0, 1])
        """
        r, g, b = rgb[:, :, 0], rgb[:, :, 1], rgb[:, :, 2]
        
        maxc    = np.maximum(np.maximum(r, g), b)
        minc    = np.minimum(np.minimum(r, g), b)
        delta   = maxc - minc
        
        # Value
        v       = maxc
        
        # Saturation
        s       = np.where(maxc != 0, delta / maxc, 0)
        
        # Hue
        h       = np.zeros_like(maxc)
        
        # Red is max
        mask    = (maxc == r) & (delta != 0)
        h[mask] = 60 * (((g[mask] - b[mask]) / delta[mask]) % 6)
        
        # Green is max
        mask    = (maxc == g) & (delta != 0)
        h[mask] = 60 * (((b[mask] - r[mask]) / delta[mask]) + 2)
        
        # Blue is max
        mask    = (maxc == b) & (delta != 0)
        h[mask] = 60 * (((r[mask] - g[mask]) / delta[mask]) + 4)
        
        hsv     = np.stack([h, s, v], axis = 2)
        
        return hsv


    def _analyze_saturation(self, hsv: np.ndarray) -> tuple[float, dict]:
        """
        Analyze saturation distribution for anomalies
        
        Real photos: Most pixels have moderate saturation (0.2-0.7)
        AI images: Can have too many highly saturated pixels (>0.8)
        
        Arguments:
        ----------
            hsv { np.ndarray } : HSV image
        
        Returns:
        --------
            { tuple }          : A tuple containing:
                                 - Suspicion score [0.0, 1.0]
                                 - Saturation Stats
        """
        saturation          = hsv[:, :, 1]
        
        if (np.mean(saturation) < 0.05):
            logger.debug("Low global saturation; skipping saturation analysis")
            return COLOR_ANALYSIS_PARAMS.NEUTRAL_SCORE, {"reason": "insufficient_color_information"}

        # Compute saturation statistics
        mean_sat            = np.mean(saturation)
        high_sat_ratio      = np.mean(saturation > COLOR_ANALYSIS_PARAMS.SAT_HIGH_THRESHOLD)
        very_high_sat_ratio = np.mean(saturation > COLOR_ANALYSIS_PARAMS.SAT_VERY_HIGH_THRESHOLD)
        
        # Overall saturation level Analysis
        mean_anomaly        = 0.0
        
        if (mean_sat > COLOR_ANALYSIS_PARAMS.SAT_MEAN_THRESHOLD):
            mean_anomaly = min(1.0, (mean_sat - COLOR_ANALYSIS_PARAMS.SAT_MEAN_THRESHOLD) * COLOR_ANALYSIS_PARAMS.SAT_MEAN_SCALE)
        
        # High saturation pixels Analysis
        high_sat_anomaly = 0.0
        
        if (high_sat_ratio > COLOR_ANALYSIS_PARAMS.HIGH_SAT_RATIO_THRESHOLD):
            high_sat_anomaly = min(1.0, (high_sat_ratio - COLOR_ANALYSIS_PARAMS.HIGH_SAT_RATIO_THRESHOLD) * COLOR_ANALYSIS_PARAMS.HIGH_SAT_SCALE)

        # Very high saturation Analysis (clipping)
        clip_anomaly = 0.0
        
        if (very_high_sat_ratio > COLOR_ANALYSIS_PARAMS.CLIP_RATIO_THRESHOLD):
            clip_anomaly = min(1.0, (very_high_sat_ratio - COLOR_ANALYSIS_PARAMS.CLIP_RATIO_THRESHOLD) * COLOR_ANALYSIS_PARAMS.CLIP_SCALE)

        # Combine Scores
        weights          = COLOR_ANALYSIS_PARAMS.SAT_SUBMETRIC_WEIGHTS

        color_score      = (weights['mean_anomaly'] * mean_anomaly + weights['high_sat_anomaly'] * high_sat_anomaly + weights['clip_anomaly'] * clip_anomaly)

        final_score      = float(np.clip(color_score, 0.0, 1.0))

        saturation_stats = {"mean_saturation"     : float(mean_sat),
                            "high_sat_ratio"      : float(high_sat_ratio),
                            "very_high_sat_ratio" : float(very_high_sat_ratio),
                            "mean_anomaly"        : float(mean_anomaly),
                            "high_sat_anomaly"    : float(high_sat_anomaly),
                            "clip_anomaly"        : float(clip_anomaly),
                           }

        logger.debug(f"Saturation - mean: {mean_sat:.3f}, high_ratio: {high_sat_ratio:.3f}, clip_ratio: {very_high_sat_ratio:.3f}")
        
        return final_score, saturation_stats


    def _analyze_color_histogram(self, rgb: np.ndarray) -> tuple[float, dict]:
        """
        Analyze RGB histogram distributions for anomalies
        
        Arguments:
        ----------
            rgb { np.ndarray } : RGB image normalized to [0, 1]
        
        Returns:
        --------
            { tuple }          : A tuple containing:
                                 - Suspicion score [0.0, 1.0]
                                 - Histogram Analysis stats
        """
        anomalies      = list()
        roughness_vals = list()
        low_clip_vals  = list()
        high_clip_vals = list()
        
        for channel_idx, channel_name in enumerate(['R', 'G', 'B']):
            channel = rgb[:, :, channel_idx]
            
            # Compute histogram
            hist, bins = np.histogram(channel, 
                                      bins  = COLOR_ANALYSIS_PARAMS.HISTOGRAM_BINS, 
                                      range = COLOR_ANALYSIS_PARAMS.HISTOGRAM_RANGE,
                                     )

            hist       = hist / (np.sum(hist) + 1e-10)
            
            # Measure histogram roughness
            hist_diff  = np.abs(np.diff(hist))
            roughness  = np.mean(hist_diff)
            roughness_vals.append(roughness)

            # High roughness = suspicious
            if (roughness > COLOR_ANALYSIS_PARAMS.ROUGHNESS_THRESHOLD):
                anomalies.append(np.clip(((roughness - COLOR_ANALYSIS_PARAMS.ROUGHNESS_THRESHOLD) * COLOR_ANALYSIS_PARAMS.ROUGHNESS_SCALE), 0.0, 1.0))
            
            # Check for clipping (peaks at extremes)
            low_clip  = hist[0] + hist[1]
            high_clip = hist[-1] + hist[-2]

            # Append values to their respective storages
            low_clip_vals.append(low_clip)
            high_clip_vals.append(high_clip)
            
            if (low_clip > COLOR_ANALYSIS_PARAMS.CLIP_THRESHOLD):
                # More than 10% near black
                anomalies.append(min(1.0, (low_clip - COLOR_ANALYSIS_PARAMS.CLIP_THRESHOLD) * COLOR_ANALYSIS_PARAMS.CLIP_SCALE_FACTOR))

            if (high_clip > COLOR_ANALYSIS_PARAMS.CLIP_THRESHOLD):
                # More than 10% near white
                anomalies.append(min(1.0, (high_clip - COLOR_ANALYSIS_PARAMS.CLIP_THRESHOLD) * COLOR_ANALYSIS_PARAMS.CLIP_SCALE_FACTOR))

        if (len(anomalies) == 0):
            logger.debug("No color histogram anomalies detected")
            return COLOR_ANALYSIS_PARAMS.NEUTRAL_SCORE, {"reason": "insufficient_color_information"}
        
        # Take mean of detected anomalies
        score           = np.mean(anomalies)
        final_score     = float(np.clip(score, 0.0, 1.0))

        histogram_stats = {"roughness_mean"    : float(np.mean(roughness_vals)),
                           "low_clip_mean"     : float(np.mean(low_clip_vals)),
                           "high_clip_mean"    : float(np.mean(high_clip_vals)),
                           "channels_analyzed" : 3,
                          }
        
        return final_score, histogram_stats


    def _analyze_hue_distribution(self, hsv: np.ndarray) -> tuple[float, dict]:
        """
        Analyze hue distribution for unnatural patterns
        
        Arguments:
        ----------
            hsv { np.ndarray } : HSV image
        
        Returns:
        --------
            { tuple }          : A tuple containing:
                                 - Suspicion score [0.0, 1.0]
                                 - hue analysis stats
        """
        hue            = hsv[:, :, 0]
        saturation     = hsv[:, :, 1]
        
        # Only consider pixels with sufficient saturation (avoid gray)
        saturated_mask = saturation > COLOR_ANALYSIS_PARAMS.HUE_SAT_MASK_THRESHOLD
        
        if (np.sum(saturated_mask) < COLOR_ANALYSIS_PARAMS.HUE_MIN_PIXELS):
            # Not enough colored pixels to analyze
            return COLOR_ANALYSIS_PARAMS.NEUTRAL_SCORE, {"reason": "insufficient_color_information"}
        
        hue_saturated         = hue[saturated_mask]

        # Prevents false positives on monotone objects
        if (np.ptp(hue_saturated) < 5.0):
            logger.debug("Hue range too narrow; returning neutral score")
            return COLOR_ANALYSIS_PARAMS.NEUTRAL_SCORE, {"reason": "insufficient_color_information"}
        
        # Compute hue histogram
        hist, bins            = np.histogram(a     = hue_saturated, 
                                             bins  = COLOR_ANALYSIS_PARAMS.HUE_BINS, 
                                             range = COLOR_ANALYSIS_PARAMS.HUE_RANGE,
                                            )

        hist                  = hist / (np.sum(hist) + 1e-10)
        
        # Unnatural hue concentration Analysis
        sorted_hist           = np.sort(hist)[::-1]
        top3_concentration    = np.sum(sorted_hist[:3])
        concentration_anomaly = 0.0
         
        if (top3_concentration > COLOR_ANALYSIS_PARAMS.HUE_CONCENTRATION_THRESHOLD): 
            # More than 60% in 3 hue bins
            concentration_anomaly = min(1.0, (top3_concentration - COLOR_ANALYSIS_PARAMS.HUE_CONCENTRATION_THRESHOLD) * COLOR_ANALYSIS_PARAMS.HUE_CONCENTRATION_SCALE)
        
        # Hue gaps Analysis
        zero_bins             = np.sum(hist < COLOR_ANALYSIS_PARAMS.HUE_EMPTY_BIN_THRESHOLD)
        gap_ratio             = zero_bins / len(hist)
        gap_anomaly           = 0.0
        
        if (gap_ratio > COLOR_ANALYSIS_PARAMS.HUE_GAP_RATIO_THRESHOLD):  
            # More than 40% empty bins
            gap_anomaly = min(1.0, (gap_ratio - COLOR_ANALYSIS_PARAMS.HUE_GAP_RATIO_THRESHOLD) * COLOR_ANALYSIS_PARAMS.HUE_GAP_SCALE)
        
        weights               = COLOR_ANALYSIS_PARAMS.HUE_SUBMETRIC_WEIGHTS
        score                 = (weights['concentration_anomaly'] * concentration_anomaly + weights['gap_anomaly'] * gap_anomaly)
        final_score           = float(np.clip(score, 0.0, 1.0))

        hue_stats             = {"top3_concentration"    : float(top3_concentration),
                                 "gap_ratio"             : float(gap_ratio),
                                 "concentration_anomaly" : float(concentration_anomaly),
                                 "gap_anomaly"           : float(gap_anomaly),
                                }

        logger.debug(f"Hue - concentration: {top3_concentration:.3f}, gap_ratio: {gap_ratio:.3f}")
        
        return final_score, hue_stats

#################################
# metrics/aggregator.py
#################################
# Dependencies
import time
import numpy as np
from typing import List
from pathlib import Path
from types import MappingProxyType
from utils.logger import get_logger
from config.settings import settings
from config.schemas import MetricResult
from config.constants import MetricType
from config.constants import SignalStatus
from config.schemas import AnalysisResult
from config.schemas import DetectionSignal
from config.constants import DetectionStatus
from config.constants import SIGNAL_THRESHOLDS
from utils.image_processor import ImageProcessor
from config.constants import METRIC_EXPLANATIONS
from metrics.noise_analyzer import NoiseAnalyzer
from metrics.color_analyzer import ColorAnalyzer
from metrics.texture_analyzer import TextureAnalyzer
from features.threshold_manager import ThresholdManager
from config.constants import IMAGE_RESIZE_MAX_DIMENSION
from metrics.frequency_analyzer import FrequencyAnalyzer
from metrics.gradient_field_pca import GradientFieldPCADetector


# Setup Logging
logger = get_logger(__name__)


class MetricsAggregator:
    """
    Main detector that orchestrates all detection methods

    Combines multiple unsupervised metrics:
    ----------------------------------------
    1. Gradient-Field PCA
    2. Frequency Domain Analysis (FFT)
    3. Noise Pattern Analysis
    4. Texture Analysis
    5. Color Distribution Analysis

    Note: Each metric produces a suspicion score [0.0, 1.0] : scores are combined using weighted average to produce final assessment
    """
    def __init__(self, threshold_manager: ThresholdManager | None = None):
        """
        Initialize all detectors
        """
        logger.info("Initializing AI Image Detector")

        # Optional runtime threshold manager
        self.threshold_manager           = threshold_manager
        
        self.gradient_field_pca_detector = GradientFieldPCADetector()
        self.frequency_analyzer          = FrequencyAnalyzer()
        self.noise_analyzer              = NoiseAnalyzer()
        self.texture_analyzer            = TextureAnalyzer()
        self.color_analyzer              = ColorAnalyzer()
        self.image_processor             = ImageProcessor()

        # Create detector registry
        self.detector_registry          = MappingProxyType({MetricType.GRADIENT  : ("Gradient Field PCA", self.gradient_field_pca_detector),
                                                            MetricType.FREQUENCY : ("Frequency Analysis", self.frequency_analyzer),
                                                            MetricType.NOISE     : ("Noise Analysis", self.noise_analyzer),
                                                            MetricType.TEXTURE   : ("Texture Analysis", self.texture_analyzer),
                                                            MetricType.COLOR     : ("Color Analysis", self.color_analyzer),
                                                          })
        
        # Get metric weights either from runtime UI or default to settings
        self.weights                    = (self.threshold_manager.get_metric_weights() if self.threshold_manager else settings.get_metric_weights())
        
        logger.info(f"Metric weights: {self.weights}")


    def analyze_image(self, image_path: Path, filename: str, image_size: tuple) -> AnalysisResult:
        """
        Analyze single image for AI generation
        
        Arguments:
        ----------
            image_path { Path }  : Path to image file
            
            filename   { str }   : Original filename
            
            image_size { tuple } : (width, height) tuple
        
        Returns:
        --------
            { AnalysisResult }   : AnalysisResult with detection outcome
        """
        logger.info(f"Analyzing image: {filename}")
        
        start_time = time.time()
        
        try:
            # Load image
            image           = self.image_processor.load_image(file_path = image_path)
            
            # Resize if needed for performance
            image           = self.image_processor.resize_if_needed(image         = image, 
                                                                    max_dimension = IMAGE_RESIZE_MAX_DIMENSION,
                                                                   )
            
            # Run all detectors and get raw scores
            metric_results  = self._run_all_detectors(image = image)
            
            # Create signals from scores (aggregator's responsibility)
            signals         = self._create_signals_from_scores(metric_results = metric_results)
            
            # Aggregate results
            overall_score   = self._aggregate_scores(metric_results = metric_results)
            
            # Determine status
            status          = self._determine_status(overall_score = overall_score)
            
            # Calculate processing time
            processing_time = time.time() - start_time
            
            # Create result
            result          = AnalysisResult(filename        = filename,
                                             overall_score   = overall_score,
                                             status          = status,
                                             confidence      = int(overall_score * 100),
                                             signals         = signals,
                                             metric_results  = metric_results,
                                             processing_time = processing_time,
                                             image_size      = image_size,
                                            )
            
            logger.info(f"Analysis complete for {filename}: status={status.value}, score={overall_score:.3f}, time={processing_time:.2f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"Analysis failed for {filename}: {e}")
            raise


    def _run_all_detectors(self, image: np.ndarray) -> dict[MetricType, MetricResult]:
        """
        Run all detection methods and collect raw scores
        
        Arguments:
        ----------
            image { np.ndarray } : RGB image array
        
        Returns:
        --------
                  { dict }       : Dictionary mapping MetricType to MetricResult
        """
        metric_results = dict()
        
        # Run eaach detector one by one
        for metric_type, (detector_name, detector) in self.detector_registry.items():
            try:
                result                      = detector.detect(image = image)
                result.metric_type          = metric_type
                metric_results[metric_type] = result
                
                logger.debug(f"{detector_name} | {metric_type.value} | score={result.score:.3f} | confidence={result.confidence:.3f}")
                
            except Exception as e:
                logger.error(f"{detector.__class__.__name__} failed: {e}")

                # Same Failure Score by all metrics with same confidence 
                metric_results[metric_type] = MetricResult(metric_type = metric_type,
                                                           score       = settings.REVIEW_THRESHOLD,
                                                           confidence  = 0.0,
                                                           details     = {"error": "detector_failed"},
                                                          )
        
        return metric_results


    def _create_signals_from_scores(self, metric_results: dict) -> List[DetectionSignal]:
        """
        Convert MetricResults to DetectionSignals with status and explanations
        
        This is the aggregator's responsibility - metrics don't know about signals
        
        Arguments:
        ----------
            metric_results { dict }   : Dictionary mapping MetricType to float score
        
        Returns:
        --------
                    { list }          : List of complete detection signals
        """
        signals           = list()

        signal_thresholds = (self.threshold_manager.get_signal_thresholds() if self.threshold_manager else SIGNAL_THRESHOLDS)
        
        for metric_type, result in metric_results.items():
            # Extract score of the metric
            score = result.score

            # Determine status based on thresholds
            if (score >= signal_thresholds[SignalStatus.FLAGGED]):
                status   = SignalStatus.FLAGGED
                severity = 'high'
                
            elif (score >= signal_thresholds[SignalStatus.WARNING]):
                status   = SignalStatus.WARNING
                severity = 'moderate'
                
            else:
                status   = SignalStatus.PASSED
                severity = 'normal'
            
            # Get explanation from constants
            explanation = METRIC_EXPLANATIONS[metric_type][severity]
            
            # Create signal
            signal      = DetectionSignal(name        = self.detector_registry[metric_type][0],
                                          metric_type = metric_type,
                                          score       = score,
                                          status      = status,
                                          explanation = explanation,
                                         )
            
            signals.append(signal)
        
        # Sort signals by score (highest first)
        signals.sort(key = lambda s: s.score, reverse = True)
        
        return signals


    def _aggregate_scores(self, metric_results: dict) -> float:
        """
        Aggregate individual metric scores using weighted average
        
        Arguments:
        ----------
            metric_results { dict } : Dictionary mapping MetricType to float score
        
        Returns:
        --------
                { float }           : Overall suspicion score [0.0, 1.0]
        """
        total_score  = 0.0
        total_weight = 0.0
        
        for metric_type, result in metric_results.items():
            weight        = self.weights.get(metric_type, 0.0)
            total_score  += result.score * weight
            total_weight += weight
        
        
        # Get Aggregated Score
        if (total_weight > 0):
            # Normalize
            overall_score = total_score / total_weight
        
        else:
            # Neutral if no valid weights
            overall_score = 0.5  
        
        logger.debug(f"Aggregated score: {overall_score:.3f}")
        
        return float(np.clip(overall_score, 0.0, 1.0))


    def _determine_status(self, overall_score: float) -> DetectionStatus:
        """
        Determine binary status from overall score
        
        Arguments:
        ----------
            overall_score { float } : Aggregated suspicion score
        
        Returns:
        --------
            { DetectionStatus }     : LIKELY_AUTHENTIC or REVIEW_REQUIRED
        """
        # Extract review threshold either from threshold_manager or deault to settings value
        review_threshold = (self.threshold_manager.get_review_threshold() if self.threshold_manager else settings.REVIEW_THRESHOLD)

        if (overall_score >= review_threshold):
            return DetectionStatus.REVIEW_REQUIRED
        
        else:
            return DetectionStatus.LIKELY_AUTHENTIC

##############################
# features/batch_processor.py
##############################
# Dependencies
import time
from typing import List
from typing import Dict
from typing import Tuple
from pathlib import Path
from typing import Callable
from utils.logger import get_logger
from config.settings import settings
from config.schemas import AnalysisResult
from concurrent.futures import TimeoutError
from concurrent.futures import as_completed
from config.constants import DetectionStatus
from config.schemas import BatchAnalysisResult
from metrics.aggregator import MetricsAggregator
from concurrent.futures import ThreadPoolExecutor
from features.threshold_manager import ThresholdManager


# Setup Logging
logger = get_logger(__name__)


class BatchProcessor:
    """
    Process multiple images in parallel or sequential mode
    
    Features:
    ---------
    - Parallel processing using ThreadPoolExecutor
    - Sequential fallback for single images or disabled parallel mode
    - Automatic error handling and recovery
    - Progress tracking and logging
    """
    def __init__(self, threshold_manager: ThresholdManager):
        """
        Initialize Batch Processor
        """
        # Instantiate threshold manager
        self.threshold_manager = threshold_manager

        # Initialize aggregator
        self.aggregator        = MetricsAggregator(threshold_manager = threshold_manager)
            
        # Fix number of workers 
        self.max_workers       = settings.MAX_WORKERS if settings.PARALLEL_PROCESSING else 1
        
        logger.info(f"BatchProcessor initialized with max_workers={self.max_workers}, parallel={settings.PARALLEL_PROCESSING}")
    

    def process_batch(self, image_files: List[Dict[str, any]], on_progress: Callable[[int, int, str], None] | None = None) -> BatchAnalysisResult:
        """
        Process multiple images with automatic parallel/sequential switching
        
        Arguments:
        ----------
            image_files   { list }    : List of dicts with keys:
                                        - 'path'     : Path object
                                        - 'filename' : str
                                        - 'size'     : tuple (width, height)

            on_progress { Callablel } : Optional callback invoked after each image is processed
        
        Returns:
        --------
            { BatchAnalysisResult } : Complete batch analysis result
        """
        start_time   = time.time()
        total_images = len(image_files)
        
        logger.info(f"Starting batch processing of {total_images} images")
        
        # Validate input
        if (total_images == 0):
            logger.warning("Empty batch provided")
            return self._create_empty_batch_result()
        
        if (total_images > settings.MAX_BATCH_SIZE):
            logger.error(f"Batch size {total_images} exceeds maximum {settings.MAX_BATCH_SIZE}")
            raise ValueError(f"Batch size {total_images} exceeds maximum allowed {settings.MAX_BATCH_SIZE}")
        
        # Choose processing strategy
        if (settings.PARALLEL_PROCESSING and (total_images > 1)):
            results, failed = self._process_parallel(image_files = image_files,
                                                     on_progress = on_progress,
                                                    )
        
        else:
            results, failed = self._process_sequential(image_files = image_files,
                                                       on_progress = on_progress,
                                                      )
        
        total_time           = time.time() - start_time
        
        # Create batch result
        batch_result         = BatchAnalysisResult(total_images          = total_images,
                                                   processed             = len(results),
                                                   failed                = failed,
                                                   results               = results,
                                                   total_processing_time = total_time,
                                                  )
        
        # Calculate summary statistics
        batch_result.summary = self._calculate_summary(results = results,
                                                       total   = total_images,
                                                      )
        
        logger.info(f"Batch processing complete: {len(results)}/{total_images} successful, {failed} failed in {total_time:.2f}s")
        
        return batch_result
    

    def _process_parallel(self, image_files: List[Dict], on_progress: Callable[[int, int, str], None] | None = None) -> Tuple[List[AnalysisResult], int]:
        """
        Process images in parallel using ThreadPoolExecutor
        
        Arguments:
        ----------
            image_files   { list }    : List of image file dictionaries

            on_progress { Callablel } : Optional callback invoked after each image is processed
        
        Returns:
        --------
            { tuple }            : (results_list, failed_count)
        """
        results = list()
        failed  = 0
        
        logger.debug(f"Using parallel processing with {self.max_workers} workers")
        
        with ThreadPoolExecutor(max_workers = self.max_workers) as executor:
            # Submit all tasks
            future_to_file = {executor.submit(self.process_single,
                                              image['path'],
                                              image['filename'],
                                              image['size'],
                                             ): image for image in image_files
                             }
            
            # Collect results as they complete
            completed = 0

            for future in as_completed(future_to_file):
                completed += 1
                image      = future_to_file[future]

                if on_progress:
                    on_progress(completed, len(image_files), image["filename"])
                
                try:
                    result = future.result(timeout = settings.PROCESSING_TIMEOUT)
                    
                    if result:
                        results.append(result)
                        logger.debug(f"✓ Completed: {image['filename']}")
                    
                    else:
                        failed += 1
                        logger.warning(f"✗ Failed: {image['filename']} (returned None)")
                
                except TimeoutError:
                    failed += 1
                    logger.error(f"✗ Timeout: {image['filename']} (exceeded {settings.PROCESSING_TIMEOUT}s)")
                
                except Exception as e:
                    failed += 1
                    logger.error(f"✗ Error: {image['filename']} - {e}")
        
        return results, failed
    

    def _process_sequential(self, image_files: List[Dict], on_progress: Callable[[int, int, str], None] | None = None) -> Tuple[List[AnalysisResult], int]:
        """
        Process images sequentially (fallback or single image)
        
        Arguments:
        ----------
            image_files   { list }   : List of image file dictionaries

            on_progress { Callabel } : Optional callback invoked after each image is processed
        
        Returns:
        --------
            { tuple }            : (results_list, failed_count)
        """
        results = list()
        failed  = 0
        
        logger.debug("Using sequential processing")
        
        for idx, image in enumerate(image_files, 1):
            try:
                if on_progress:
                    on_progress(idx, len(image_files), image["filename"])
                
                result = self.process_single(image_path = image['path'],
                                             filename   = image['filename'],
                                             image_size = image['size'],
                                            )
                
                if result:
                    results.append(result)
                    logger.debug(f"✓ Completed: {image['filename']}")
                
                else:
                    failed += 1
                    logger.warning(f"✗ Failed: {image['filename']} (returned None)")
            
            except Exception as e:
                failed += 1
                logger.error(f"✗ Error: {image['filename']} - {e}")
        
        return results, failed
    

    def process_single(self, image_path: Path, filename: str, image_size: Tuple[int, int]) -> AnalysisResult:
        """
        Process single image (called by both parallel and sequential)
        
        Arguments:
        ----------
            image_path { Path }  : Path to image file
            
            filename   { str }   : Original filename
            
            image_size { tuple } : (width, height)
        
        Returns:
        --------
            { AnalysisResult }   : Analysis result or None on error
        """
        try:
            return self.aggregator.analyze_image(image_path = image_path,
                                                 filename   = filename,
                                                 image_size = image_size,
                                                )
        
        except Exception as e:
            logger.error(f"Failed to process {filename}: {e}", exc_info = True)
            return None
    

    def _calculate_summary(self, results: List[AnalysisResult], total: int) -> Dict[str, int]:
        """
        Calculate summary statistics from results
        
        Arguments:
        ----------
            results { list } : List of analysis results
            
            total   { int }  : Total number of images
        
        Returns:
        --------
            { dict }         : Summary statistics
        """
        # Calculate processing stats
        likely_authentic = sum(1 for r in results if (r.status == DetectionStatus.LIKELY_AUTHENTIC))
        review_required  = sum(1 for r in results if (r.status == DetectionStatus.REVIEW_REQUIRED))

        processed        = len(results)
        failed           = total - processed
        success_rate     = int((processed / total * 100) if (total > 0) else 0)
        
        # Calculate average scores
        avg_score        = sum(r.overall_score for r in results) / len(results) if results else 0.0
        avg_confidence   = sum(r.confidence for r in results) / len(results) if results else 0
        avg_proc_time    = sum(r.processing_time for r in results) / len(results) if results else 0.0
        
        return {"likely_authentic" : likely_authentic,
                "review_required"  : review_required,
                "success_rate"     : success_rate,
                "processed"        : processed,
                "failed"           : failed,
                "avg_score"        : round(avg_score, 3),
                "avg_confidence"   : int(avg_confidence),
                "avg_proc_time"    : round(avg_proc_time, 2),
               }
    

    def _create_empty_batch_result(self) -> BatchAnalysisResult:
        """
        Create empty batch result for edge cases
        
        Returns:
        --------
            { BatchAnalysisResult } : Empty batch result
        """
        return BatchAnalysisResult(total_images          = 0,
                                   processed             = 0,
                                   failed                = 0,
                                   results               = [],
                                   summary               = {"likely_authentic" : 0,
                                                            "review_required"  : 0,
                                                            "success_rate"     : 0,
                                                           },
                                   total_processing_time = 0.0,
                                  )

################################
# features/threshold_manager.py
################################
# Dependencies
from typing import Dict
from utils.logger import get_logger
from config.settings import settings
from config.constants import MetricType
from config.constants import SignalStatus
from config.constants import SIGNAL_THRESHOLDS


# Setup Logging
logger = get_logger(__name__)


class ThresholdManager:
    """
    Manage detection thresholds dynamically
    
    Purpose:
    --------
    Allows runtime adjustment of detection thresholds for:
    - A/B testing different sensitivity levels
    - Calibration based on real-world performance
    - Custom thresholds for specific use cases
    - Environment-specific tuning (production vs staging)
    
    Note: Changes are runtime-only and not persisted
    """
    def __init__(self):
        """
        Initialize Threshold Manager with current settings
        """
        self._review_threshold  = settings.REVIEW_THRESHOLD
        self._signal_thresholds = dict(SIGNAL_THRESHOLDS)
        self._metric_weights    = dict(settings.get_metric_weights())
        
        logger.info(f"ThresholdManager initialized: review_threshold={self._review_threshold}")
    

    def get_review_threshold(self) -> float:
        """
        Get current review threshold
        
        Returns:
        --------
            { float } : Current threshold [0.0, 1.0]
        """
        return self._review_threshold
    

    def set_review_threshold(self, new_threshold: float) -> bool:
        """
        Set new review threshold
        
        Arguments:
        ----------
            new_threshold { float } : New threshold value [0.0, 1.0]
        
        Returns:
        --------
            { bool }                : Success status
        """
        if not (0.0 <= new_threshold <= 1.0):
            logger.error(f"Invalid threshold: {new_threshold} (must be between 0.0 and 1.0)")
            return False
        
        old_threshold          = self._review_threshold
        self._review_threshold = new_threshold
        
        logger.info(f"Review threshold changed: {old_threshold:.2f} → {new_threshold:.2f}")
        
        return True
    

    def adjust_sensitivity(self, sensitivity: str) -> bool:
        """
        Adjust sensitivity using preset levels
        
        Arguments:
        ----------
            sensitivity { str } : One of 'conservative', 'balanced', 'aggressive'
        
        Returns:
        --------
            { bool }            : Success status
        """
        presets = {'conservative' : 0.75,  # Fewer false positives, may miss some AI
                   'balanced'     : 0.65,  # Recommended default
                   'aggressive'   : 0.55,  # Catch more AI, more false positives
                  }
        
        if (sensitivity not in presets):
            logger.error(f"Invalid sensitivity: {sensitivity}. Must be one of {list(presets.keys())}")
            return False
        
        new_threshold = presets[sensitivity]
        success       = self.set_review_threshold(new_threshold = new_threshold)
        
        if success:
            logger.info(f"Sensitivity set to '{sensitivity}' (threshold={new_threshold})")
        
        return success
    

    def get_signal_thresholds(self) -> Dict[SignalStatus, float]:
        """
        Get current signal thresholds
        
        Returns:
        --------
            { dict } : Signal status → threshold mapping
        """
        return self._signal_thresholds.copy()
    

    def set_signal_threshold(self, status: SignalStatus, threshold: float) -> bool:
        """
        Set threshold for specific signal status
        
        Arguments:
        ----------
            status    { SignalStatus } : Signal status to modify
            
            threshold { float }        : New threshold [0.0, 1.0]
        
        Returns:
        --------
            { bool }                   : Success status
        """
        if not (0.0 <= threshold <= 1.0):
            logger.error(f"Invalid threshold: {threshold}")
            return False
        
        old_threshold                    = self._signal_thresholds.get(status)
        self._signal_thresholds[status]  = threshold
        
        logger.info(f"Signal threshold for {status.value}: {old_threshold:.2f} → {threshold:.2f}")
        
        return True
    

    def get_metric_weights(self) -> Dict[MetricType, float]:
        """
        Get current metric weights
        
        Returns:
        --------
            { dict } : Metric type → weight mapping
        """
        return self._metric_weights.copy()
    

    def set_metric_weight(self, metric: MetricType, weight: float) -> bool:
        """
        Set weight for specific metric
        
        Arguments:
        ----------
            metric { MetricType } : Metric to modify
            
            weight   { float }    : New weight [0.0, 1.0]
        
        Returns:
        --------
            { bool }              : Success status
        """
        if not (0.0 <= weight <= 1.0):
            logger.error(f"Invalid weight: {weight}")
            return False
        
        old_weight                   = self._metric_weights.get(metric, 0.0)
        self._metric_weights[metric] = weight
        
        # Validate total weight
        total_weight                 = sum(self._metric_weights.values())
        
        if not (0.99 <= total_weight <= 1.01):
            logger.warning(f"Total metric weights = {total_weight:.3f} (should sum to 1.0)")
        
        logger.info(f"Metric weight for {metric.value}: {old_weight:.2f} → {weight:.2f}")
        
        return True
    

    def set_all_metric_weights(self, weights: Dict[MetricType, float]) -> bool:
        """
        Set all metric weights at once (ensures sum = 1.0)
        
        Arguments:
        ----------
            weights { dict } : Complete metric weights mapping
        
        Returns:
        --------
            { bool }         : Success status
        """
        # Validate input
        if (not all(0.0 <= w <= 1.0 for w in weights.values())):
            logger.error("All weights must be between 0.0 and 1.0")
            return False
        
        total_weight         = sum(weights.values())
        
        if not (0.99 <= total_weight <= 1.01):
            logger.error(f"Weights must sum to 1.0, got {total_weight:.3f}")
            return False
        
        self._metric_weights = dict(weights)
        
        logger.info(f"All metric weights updated: {self._metric_weights}")
        
        return True
    

    def get_recommendations(self, score: float) -> Dict[str, str]:
        """
        Get action recommendations based on score
        
        Arguments:
        ----------
            score { float } : Overall suspicion score [0.0, 1.0]
        
        Returns:
        --------
            { dict }        : Recommendation details
        """
        if (score >= 0.85):
            return {"priority"   : "HIGH",
                    "action"     : "Immediate manual verification recommended",
                    "confidence" : "Very high likelihood of AI generation",
                    "next_steps" : "Forensic analysis, reverse image search, metadata inspection",
                   }
        
        elif (score >= 0.70):
            return {"priority"   : "MEDIUM",
                    "action"     : "Manual verification recommended",
                    "confidence" : "High likelihood of AI generation",
                    "next_steps" : "Visual inspection, compare with similar authentic images",
                   }
        
        elif (score >= 0.50):
            return {"priority"   : "LOW",
                    "action"     : "Optional review",
                    "confidence" : "Moderate indicators of AI generation",
                    "next_steps" : "May be heavily edited real photo, check source",
                   }
        
        else:
            return {"priority"   : "NONE",
                    "action"     : "No immediate action needed",
                    "confidence" : "Low likelihood of AI generation",
                    "next_steps" : "Likely authentic, proceed normally",
                   }
    

    def get_current_config(self) -> Dict[str, object]:
        """
        Get complete current configuration
        
        Returns:
        --------
            { dict } : All current threshold and weight settings
        """
        return {"review_threshold"  : self._review_threshold,
                "signal_thresholds" : self._signal_thresholds.copy(),
                "metric_weights"    : self._metric_weights.copy(),
               }
    

    def reset_to_defaults(self) -> None:
        """
        Reset all thresholds to default settings
        """
        self._review_threshold  = settings.REVIEW_THRESHOLD
        self._signal_thresholds = dict(SIGNAL_THRESHOLDS)
        self._metric_weights    = dict(settings.get_metric_weights())
        
        logger.info("All thresholds reset to default values")

#####################################
# features/detailed_result_maker.py
#####################################
# Dependencies
import pandas as pd
from typing import Dict
from typing import List
from typing import Optional
from utils.logger import get_logger
from config.constants import MetricType
from config.constants import SignalStatus
from config.schemas import AnalysisResult
from config.constants import SIGNAL_THRESHOLDS


# Setup Logging
logger = get_logger(__name__)


class DetailedResultMaker:
    """
    Extract and format detailed analysis results for UI and reporting
    
    Purpose:
    --------
    - Extracts all intermediate metrics from MetricResult objects
    - Formats data for tabular display in UI
    - Provides rich metadata for PDF/CSV reports
    - No re-computation - just data extraction and formatting
    
    Output Formats:
    ---------------
    1. Structured dictionaries for UI
    2. Pandas DataFrames for reports
    3. Hierarchical JSON for API 
    """
    def __init__(self, signal_thresholds: dict | None = None):
        """
        Initialize Detailed Result Maker
        """
        self.metric_display_names = {MetricType.GRADIENT  : "Gradient-Field PCA",
                                     MetricType.FREQUENCY : "Frequency Domain (FFT)",
                                     MetricType.NOISE     : "Noise Pattern Analysis",
                                     MetricType.TEXTURE   : "Texture Statistics",
                                     MetricType.COLOR     : "Color Distribution",
                                    }

        self.signal_thresholds    = signal_thresholds or SIGNAL_THRESHOLDS
        
        logger.debug("DetailedResultMaker initialized")
    

    def extract_detailed_results(self, analysis_result: AnalysisResult) -> Dict:
        """
        Extract all detailed results from AnalysisResult
        
        Arguments:
        ----------
            analysis_result { AnalysisResult } : Complete analysis result
        
        Returns:
        --------
            { dict }                           : Comprehensive detailed results
        """
        logger.debug(f"Extracting detailed results for: {analysis_result.filename}")
        
        detailed = {"filename"         : analysis_result.filename,
                    "overall_summary"  : self._extract_overall_summary(analysis_result = analysis_result),
                    "metrics_detailed" : self._extract_all_metrics(analysis_result = analysis_result),
                    "metadata"         : self._extract_metadata(analysis_result = analysis_result),
                   }
        
        logger.debug(f"Extracted {len(detailed['metrics_detailed'])} metric details")
        
        return detailed
    

    def create_detailed_table(self, analysis_result: AnalysisResult) -> pd.DataFrame:
        """
        Create detailed metrics table as DataFrame
        
        Arguments:
        ----------
            analysis_result { AnalysisResult } : Complete analysis result
        
        Returns:
        --------
            { DataFrame }                      : Tabular detailed results
        """
        rows = list()
        
        for metric_type, metric_result in analysis_result.metric_results.items():
            display_name = self.metric_display_names.get(metric_type, metric_type.value)
            
            row          = {"Metric"      : display_name,
                            "Score"       : round(metric_result.score, 3),
                            "Confidence"  : round(metric_result.confidence, 3) if metric_result.confidence is not None else "N/A",
                            "Status"      : self._score_to_status(score = metric_result.score),
                           }
            
            # Add key details from each metric
            details      = self._extract_key_details(metric_type   = metric_type,
                                                     metric_result = metric_result,
                                                    )
            
            row.update(details)
            rows.append(row)
        
        # Dump rows into a pandas dataframe for structured result
        dataframe = pd.DataFrame(data = rows)
        
        logger.debug(f"Created detailed table with {len(dataframe)} rows, {len(dataframe.columns)} columns")
        
        return dataframe
    

    def create_report_data(self, analysis_result: AnalysisResult) -> Dict:
        """
        Create rich data structure for report generation
        
        Arguments:
        ----------
            analysis_result { AnalysisResult } : Complete analysis result
        
        Returns:
        --------
            { dict }                           : Report-ready data structure
        """
        report_data = {"header"             : self._create_report_header(analysis_result = analysis_result),
                       "overall_assessment" : self._create_overall_assessment(analysis_result = analysis_result),
                       "metric_breakdown"   : self._create_metric_breakdown(analysis_result = analysis_result),
                       "forensic_details"   : self._create_forensic_details(analysis_result = analysis_result),
                       "recommendations"    : self._create_recommendations(analysis_result = analysis_result),
                      }
        
        logger.debug(f"Created report data for: {analysis_result.filename}")
        
        return report_data
    

    def _extract_overall_summary(self, analysis_result: AnalysisResult) -> Dict:
        """
        Extract overall summary information
        """
        timestamp = getattr(analysis_result, "timestamp", None)

        return {"filename"        : analysis_result.filename,
                "status"          : analysis_result.status.value,
                "overall_score"   : round(analysis_result.overall_score, 3),
                "confidence"      : analysis_result.confidence,
                "processing_time" : round(analysis_result.processing_time, 2),
                "image_size"      : f"{analysis_result.image_size[0]}×{analysis_result.image_size[1]}",
                "timestamp"       : timestamp.isoformat() if timestamp else None,
               }
    

    def _extract_all_metrics(self, analysis_result: AnalysisResult) -> List[Dict]:
        """
        Extract detailed information for all metrics
        """
        metrics_detailed = list()
        
        for metric_type, metric_result in analysis_result.metric_results.items():
            metric_detail = {"metric_type"    : metric_type.value,
                             "display_name"   : self.metric_display_names.get(metric_type, metric_type.value),
                             "score"          : round(metric_result.score, 3),
                             "confidence"     : round(metric_result.confidence, 3) if metric_result.confidence is not None else None,
                             "status"         : self._score_to_status(score = metric_result.score),
                             "details"        : metric_result.details or {},
                             "interpretation" : self._interpret_metric(metric_type   = metric_type,
                                                                       metric_result = metric_result,
                                                                      ),
                            }
            
            metrics_detailed.append(metric_detail)
        
        # Sort by score (highest first)
        metrics_detailed.sort(key = lambda x: x['score'], reverse = True)
        
        return metrics_detailed
    

    def _extract_metadata(self, analysis_result: AnalysisResult) -> Dict:
        """
        Extract processing metadata
        """
        return {"total_metrics"   : len(analysis_result.metric_results),
                "flagged_metrics" : sum(1 for s in analysis_result.signals if s.status.value == 'flagged'),
                "warning_metrics" : sum(1 for s in analysis_result.signals if s.status.value == 'warning'),
                "passed_metrics"  : sum(1 for s in analysis_result.signals if s.status.value == 'passed'),
                "avg_confidence"  : self._calculate_avg_confidence(analysis_result = analysis_result),
               }
    

    def _extract_key_details(self, metric_type: MetricType, metric_result) -> Dict:
        """
        Extract key details specific to each metric type
        """
        details = metric_result.details or {}
        
        if (metric_type == MetricType.GRADIENT):
            return {"Eigenvalue_Ratio" : details.get('eigenvalue_ratio', 'N/A'),
                    "Vectors_Sampled"  : details.get('gradient_vectors_sampled', 'N/A'),
                   }
        
        elif (metric_type == MetricType.FREQUENCY):
            return {"HF_Ratio"        : details.get('hf_ratio', 'N/A'),
                    "HF_Anomaly"      : details.get('hf_anomaly', 'N/A'),
                    "Spectrum_Bins"   : details.get('spectrum_bins', 'N/A'),
                   }
        
        elif (metric_type == MetricType.NOISE):
            return {"Mean_Noise"      : details.get('mean_noise', 'N/A'),
                    "CV"              : details.get('cv', 'N/A'),
                    "Patches_Valid"   : details.get('patches_valid', 'N/A'),
                   }
        
        elif (metric_type == MetricType.TEXTURE):
            return {"Smooth_Ratio"    : details.get('smooth_ratio', 'N/A'),
                    "Contrast_Mean"   : details.get('contrast_mean', 'N/A'),
                    "Patches_Used"    : details.get('patches_used', 'N/A'),
                   }
        
        elif (metric_type == MetricType.COLOR):
            sat_stats = details.get('saturation_stats', {})
            return {"Mean_Saturation" : sat_stats.get('mean_saturation', 'N/A'),
                    "High_Sat_Ratio"  : sat_stats.get('high_sat_ratio', 'N/A'),
                   }
        
        return {}
    

    def _interpret_metric(self, metric_type: MetricType, metric_result) -> str:
        """
        Provide human-readable interpretation of metric result
        """
        score   = metric_result.score
        details = metric_result.details or {}
        
        if (metric_type == MetricType.GRADIENT):
            eig_ratio = details.get('eigenvalue_ratio')
            
            if eig_ratio:
                return f"Eigenvalue ratio of {eig_ratio:.3f} ({'high' if eig_ratio > 0.85 else 'low'} alignment)"
            
            return "Gradient structure analysis"
        
        elif (metric_type == MetricType.FREQUENCY):
            hf_ratio = details.get('hf_ratio')
            
            if hf_ratio:
                return f"High-freq ratio: {hf_ratio:.3f} ({'elevated' if hf_ratio > 0.35 else 'low' if hf_ratio < 0.08 else 'normal'})"
            
            return "Frequency spectrum analysis"
        
        elif (metric_type == MetricType.NOISE):
            mean_noise = details.get('mean_noise')
            
            if mean_noise:
                return f"Mean noise: {mean_noise:.2f} ({'low' if mean_noise < 1.5 else 'normal'})"
            
            return "Noise pattern analysis"
        
        elif (metric_type == MetricType.TEXTURE):
            smooth_ratio = details.get('smooth_ratio')
            
            if smooth_ratio is not None:
                return f"Smooth regions: {smooth_ratio:.1%} ({'excessive' if smooth_ratio > 0.4 else 'normal'})"
            
            return "Texture variation analysis"
        
        elif (metric_type == MetricType.COLOR):
            sat_stats = details.get('saturation_stats', {})
            mean_sat  = sat_stats.get('mean_saturation')
            
            if mean_sat:
                return f"Mean saturation: {mean_sat:.2f} ({'high' if mean_sat > 0.65 else 'normal'})"
            
            return "Color distribution analysis"
        
        return "Analysis complete"
    
    
    def _create_report_header(self, analysis_result: AnalysisResult) -> Dict:
        """
        Create report header section
        """
        return {"filename"        : analysis_result.filename,
                "analysis_date"   : analysis_result.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                "image_size"      : f"{analysis_result.image_size[0]} × {analysis_result.image_size[1]} pixels",
                "processing_time" : f"{analysis_result.processing_time:.2f} seconds",
               }
    

    def _create_overall_assessment(self, analysis_result: AnalysisResult) -> Dict:
        """
        Create overall assessment section
        """
        return {"status"       : analysis_result.status.value,
                "score"        : round(analysis_result.overall_score * 100, 1),
                "confidence"   : analysis_result.confidence,
                "verdict"      : "REVIEW REQUIRED" if analysis_result.status.value == "REVIEW_REQUIRED" else "LIKELY AUTHENTIC",
                "risk_level"   : self._calculate_risk_level(score = analysis_result.overall_score),
               }


    def _create_metric_breakdown(self, analysis_result: AnalysisResult) -> List[Dict]:
        """
        Create detailed metric breakdown for report
        """
        breakdown = list()
        
        for signal in analysis_result.signals:
            metric_result = analysis_result.metric_results.get(signal.metric_type)
            
            item          = {"metric"       : signal.name,
                             "score"        : f"{signal.score * 100:.1f}%",
                             "status"       : signal.status.value.upper(),
                             "confidence"   : f"{metric_result.confidence * 100:.1f}%" if metric_result.confidence else "N/A",
                             "explanation"  : signal.explanation,
                             "key_findings" : self.extract_key_findings(metric_type   = signal.metric_type,
                                                                        metric_result = metric_result,
                                                                       ),
                            }
            
            breakdown.append(item)
        
        return breakdown


    def _create_forensic_details(self, analysis_result: AnalysisResult) -> Dict:
        """
        Create forensic details section
        """
        forensic = dict()
        
        for metric_type, metric_result in analysis_result.metric_results.items():
            metric_name           = self.metric_display_names.get(metric_type, metric_type.value)
            forensic[metric_name] = metric_result.details or {"note": "No detailed forensics available"}
        
        return forensic


    def _create_recommendations(self, analysis_result: AnalysisResult) -> Dict:
        """
        Create recommendations section
        """
        score = analysis_result.overall_score
        
        if (score >= 0.85):
            return {"action"      : "Immediate manual verification required",
                    "priority"    : "HIGH",
                    "next_steps"  : ["Forensic analysis", "Reverse image search", "Metadata inspection", "Expert review"],
                    "confidence"  : "Very high likelihood of AI generation",
                   }
        
        elif (score >= 0.70):
            return {"action"      : "Manual verification recommended",
                    "priority"    : "MEDIUM",
                    "next_steps"  : ["Visual inspection", "Compare with authentic samples", "Check source provenance"],
                    "confidence"  : "High likelihood of AI generation",
                   }
        
        elif (score >= 0.50):
            return {"action"      : "Optional review suggested",
                    "priority"    : "LOW",
                    "next_steps"  : ["May be edited photo", "Verify image source", "Check for inconsistencies"],
                    "confidence"  : "Moderate indicators present",
                   }
        
        else:
            return {"action"      : "No immediate action required",
                    "priority"    : "NONE",
                    "next_steps"  : ["Proceed with normal workflow"],
                    "confidence"  : "Low likelihood of AI generation",
                   }


    def _score_to_status(self, score: float) -> str:
        """
        Convert score to status label
        """
        if (score >= self.signal_thresholds[SignalStatus.FLAGGED]):
            return "FLAGGED"

        elif (score >= self.signal_thresholds[SignalStatus.WARNING]):
            return "WARNING"
        
        else:
            return "PASSED"


    def _calculate_avg_confidence(self, analysis_result: AnalysisResult) -> float:
        """
        Calculate average confidence across all metrics
        """
        confidences = [mr.confidence for mr in analysis_result.metric_results.values() if mr.confidence is not None]
        
        return round(sum(confidences) / len(confidences), 3) if confidences else 0.0


    def _calculate_risk_level(self, score: float) -> str:
        """
        Calculate risk level from score
        """
        if (score >= 0.85):
            return "CRITICAL"

        elif (score >= 0.70):
            return "HIGH"
        
        elif (score >= 0.50):
            return "MEDIUM"
        
        else:
            return "LOW"


    def extract_key_findings(self, metric_type: MetricType, metric_result) -> List[str]:
        """
        Extract human-readable key forensic findings for a given metric used by:
        - Detailed UI views
        - CSV reports
        - JSON reports
        """
        findings = list()
        details  = metric_result.details or {}
        
        if (metric_type == MetricType.GRADIENT):
            eig_ratio = details.get('eigenvalue_ratio')
            
            if eig_ratio:
                findings.append(f"Eigenvalue ratio: {eig_ratio:.3f}")
            
            vectors = details.get('gradient_vectors_sampled')
            
            if vectors:
                findings.append(f"Analyzed {vectors} gradient vectors")
        
        elif (metric_type == MetricType.FREQUENCY):
            hf_ratio = details.get('hf_ratio')
            
            if hf_ratio:
                findings.append(f"High-frequency ratio: {hf_ratio:.3f}")
            
            roughness = details.get('roughness')
            if roughness:
                findings.append(f"Spectral roughness: {roughness:.3f}")
        
        elif (metric_type == MetricType.NOISE):
            mean_noise = details.get('mean_noise')
            
            if mean_noise:
                findings.append(f"Mean noise level: {mean_noise:.2f}")
            
            cv = details.get('cv')
            
            if cv:
                findings.append(f"Coefficient of variation: {cv:.3f}")
        
        elif (metric_type == MetricType.TEXTURE):
            smooth_ratio = details.get('smooth_ratio')
            
            if smooth_ratio:
                findings.append(f"Smooth patches: {smooth_ratio:.1%}")
            
            contrast_mean = details.get('contrast_mean')
            
            if contrast_mean:
                findings.append(f"Average contrast: {contrast_mean:.2f}")
        
        elif (metric_type == MetricType.COLOR):
            sat_stats = details.get('saturation_stats', {})
            mean_sat  = sat_stats.get('mean_saturation')
            
            if mean_sat:
                findings.append(f"Mean saturation: {mean_sat:.2f}")
            
            high_sat = sat_stats.get('high_sat_ratio')
            
            if high_sat:
                findings.append(f"High saturation pixels: {high_sat:.1%}")
        
        return findings if findings else ["Analysis complete"]

#############################
# reporter/csv_reporter.py
#############################
# Dependencies
import csv
from pathlib import Path
from typing import Optional
from datetime import datetime
from utils.logger import get_logger
from config.settings import settings
from config.constants import MetricType
from config.schemas import AnalysisResult
from utils.helpers import generate_unique_id
from config.constants import DetectionStatus
from config.schemas import BatchAnalysisResult
from features.detailed_result_maker import DetailedResultMaker


# Setup Logging
logger = get_logger(__name__)


class CSVReporter:
    """
    Professional CSV report generator
    
    Features:
    ---------
    - Single image detailed reports
    - Batch summary reports with statistics
    - Detailed forensic data export
    - Excel-compatible formatting
    - UTF-8 encoding with BOM for international compatibility
    """
    def __init__(self):
        """
        Initialize CSV Reporter
        """
        self.detailed_maker = DetailedResultMaker()
        logger.debug("CSVReporter initialized")
    

    def export_batch_summary(self, batch_result: BatchAnalysisResult, output_dir: Optional[Path] = None) -> Path:
        """
        Export batch analysis summary as CSV
        
        Arguments:
        ----------
            batch_result { BatchAnalysisResult } : Complete batch analysis result
            
            output_dir   { Path }                : Output directory (defaults to settings.REPORTS_DIR)
        
        Returns:
        --------
                       { Path }                  : Path to generated CSV file
        """
        output_dir  = output_dir or settings.REPORTS_DIR
        report_id   = generate_unique_id()
        filename    = f"batch_summary_{report_id}.csv"
        output_path = output_dir / filename
        
        logger.info(f"Generating batch summary CSV: {filename}")
        
        try:
            with open(output_path, 'w', newline = '', encoding = 'utf-8-sig') as f:
                writer = csv.writer(f)
                
                # Report Header
                self._write_report_header(writer      = writer,
                                          report_type = "Batch Analysis Summary",
                                          timestamp   = batch_result.timestamp,
                                         )
                
                # Batch Statistics
                self._write_batch_statistics(writer       = writer,
                                             batch_result = batch_result,
                                            )
                
                # Main Results Table
                self._write_batch_results_table(writer       = writer,
                                                batch_result = batch_result,
                                               )
                
                # Footer
                self._write_footer(writer = writer)
            
            logger.info(f"Batch summary CSV generated: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"Failed to generate batch summary CSV: {e}")
            raise
    

    def export_batch_detailed(self, batch_result: BatchAnalysisResult, output_dir: Optional[Path] = None) -> Path:
        """
        Export detailed batch analysis with forensic data
        
        Arguments:
        ----------
            batch_result { BatchAnalysisResult } : Complete batch analysis result
            
            output_dir   { Path }                : Output directory (defaults to settings.REPORTS_DIR)
        
        Returns:
        --------
                      { Path }                   : Path to generated CSV file
        """
        output_dir  = output_dir or settings.REPORTS_DIR
        report_id   = generate_unique_id()
        filename    = f"batch_detailed_{report_id}.csv"
        output_path = output_dir / filename
        
        logger.info(f"Generating detailed batch CSV: {filename}")
        
        try:
            with open(output_path, 'w', newline = '', encoding = 'utf-8-sig') as f:
                writer = csv.writer(f)
                
                # Report Header
                self._write_report_header(writer      = writer,
                                          report_type = "Detailed Batch Analysis",
                                          timestamp   = batch_result.timestamp,
                                         )
                
                # Process each image with full details
                for idx, result in enumerate(batch_result.results, 1):
                    self._write_detailed_image_section(writer        = writer,
                                                       result        = result,
                                                       image_number  = idx,
                                                       total_images  = batch_result.processed,
                                                      )
                    
                    # Add separator between images
                    if (idx < batch_result.processed):
                        writer.writerow([])
                        writer.writerow(['=' * 100])
                        writer.writerow([])
                
                # Footer
                self._write_footer(writer = writer)
            
            logger.info(f"Detailed batch CSV generated: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"Failed to generate detailed batch CSV: {e}")
            raise
    

    def export_single_detailed(self, result: AnalysisResult, output_dir: Optional[Path] = None) -> Path:
        """
        Export single image detailed analysis as CSV
        
        Arguments:
        ----------
            result     { AnalysisResult } : Single image analysis result
            
            output_dir { Path }           : Output directory (defaults to settings.REPORTS_DIR)
        
        Returns:
        --------
                     { Path }             : Path to generated CSV file
        """
        output_dir  = output_dir or settings.REPORTS_DIR
        report_id   = generate_unique_id()
        filename    = f"single_analysis_{report_id}.csv"
        output_path = output_dir / filename
        
        logger.info(f"Generating single image CSV: {filename}")
        
        try:
            with open(output_path, 'w', newline = '', encoding = 'utf-8-sig') as f:
                writer = csv.writer(f)
                
                # Report Header
                self._write_report_header(writer     = writer,
                                          report_type = "Single Image Analysis",
                                          timestamp   = result.timestamp,
                                         )
                
                # Image Details
                self._write_detailed_image_section(writer       = writer,
                                                   result       = result,
                                                   image_number = 1,
                                                   total_images = 1,
                                                  )
                
                # Footer
                self._write_footer(writer = writer)
            
            logger.info(f"Single image CSV generated: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"Failed to generate single image CSV: {e}")
            raise
    

    def export_metrics_comparison(self, batch_result: BatchAnalysisResult, output_dir: Optional[Path] = None) -> Path:
        """
        Export metrics comparison table across all images
        
        Arguments:
        ----------
            batch_result { BatchAnalysisResult } : Complete batch analysis result
            
            output_dir   { Path }                : Output directory (defaults to settings.REPORTS_DIR)
        
        Returns:
        --------
                       { Path }                  : Path to generated CSV file
        """
        output_dir  = output_dir or settings.REPORTS_DIR
        report_id   = generate_unique_id()
        filename    = f"metrics_comparison_{report_id}.csv"
        output_path = output_dir / filename
        
        logger.info(f"Generating metrics comparison CSV: {filename}")
        
        try:
            with open(output_path, 'w', newline = '', encoding = 'utf-8-sig') as f:
                writer = csv.writer(f)
                
                # Report Header
                self._write_report_header(writer     = writer,
                                          report_type = "Metrics Comparison",
                                          timestamp   = batch_result.timestamp,
                                         )
                
                # Comparison Table Header
                writer.writerow(['Metrics Comparison Across All Images'])
                writer.writerow([])
                
                header = ['Filename', 
                          'Overall Score', 
                          'Analysis Status', 
                          'Gradient Analysis Score', 
                          'Gradient Analysis Confidence', 
                          'Frequency Analysis Score', 
                          'Frequency Analysis Confidence',
                          'Noise Analysis Score', 
                          'Noise Analysis Confidence',
                          'Texture Analysis Score', 
                          'Texture Analysis Confidence',
                          'Color Analysis Score', 
                          'Color Analysis Confidence',
                          'Processing Time',
                         ]
                
                writer.writerow(header)
                
                # Data rows
                for result in batch_result.results:
                    row = [result.filename,
                           f"{result.overall_score:.3f}",
                           result.status.value,
                          ]
                    
                    # Add each metric's score and confidence
                    for metric_type in [MetricType.GRADIENT, MetricType.FREQUENCY, MetricType.NOISE, MetricType.TEXTURE, MetricType.COLOR]:
                        metric_result = result.metric_results.get(metric_type)
                        
                        if metric_result:
                            row.append(f"{metric_result.score:.3f}")
                            row.append(f"{metric_result.confidence:.3f}" if metric_result.confidence is not None else "N/A")
                        
                        else:
                            row.extend(["N/A", "N/A"])
                    
                    row.append(f"{result.processing_time:.2f}s")
                    writer.writerow(row)
                
                # Footer
                writer.writerow([])
                self._write_footer(writer = writer)
            
            logger.info(f"Metrics comparison CSV generated: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"Failed to generate metrics comparison CSV: {e}")
            raise
    

    def _write_report_header(self, writer, report_type: str, timestamp: datetime) -> None:
        """
        Write CSV report header
        """
        writer.writerow(['=' * 100])
        writer.writerow([f'AI Image Screener - {report_type}'])
        writer.writerow([f'Generated: {timestamp.strftime("%Y-%m-%d %H:%M:%S")}'])
        writer.writerow([f'Version: {settings.VERSION}'])
        writer.writerow(['=' * 100])
        writer.writerow([])
    

    def _write_batch_statistics(self, writer, batch_result: BatchAnalysisResult) -> None:
        """
        Write batch statistics section
        """
        writer.writerow(['BATCH STATISTICS'])
        writer.writerow([])
        
        stats = [['Total Images', batch_result.total_images],
                 ['Successfully Processed', batch_result.processed],
                 ['Failed', batch_result.failed],
                 ['Success Rate', f"{batch_result.summary.get('success_rate', 0)}%"],
                 ['' , ''],
                 ['Likely Authentic', batch_result.summary.get('likely_authentic', 0)],
                 ['Review Required', batch_result.summary.get('review_required', 0)],
                 ['', ''],
                 ['Average Score', f"{batch_result.summary.get('avg_score', 0):.3f}"],
                 ['Average Confidence', f"{batch_result.summary.get('avg_confidence', 0)}%"],
                 ['Total Processing Time', f"{batch_result.total_processing_time:.2f}s"],
                 ['Average Time per Image', f"{batch_result.summary.get('avg_proc_time', 0):.2f}s"],
                ]
        
        for row in stats:
            writer.writerow(row)
        
        writer.writerow([])
        writer.writerow(['=' * 100])
        writer.writerow([])
    

    def _write_batch_results_table(self, writer, batch_result: BatchAnalysisResult) -> None:
        """
        Write batch results main table
        """
        writer.writerow(['ANALYSIS RESULTS'])
        writer.writerow([])
        
        # Table Header
        header = ['Filename', 
                  'Image Size',
                  'Analysis Status', 
                  'Overall Score', 
                  'Analysis Confidence (%)', 
                  'Top Warning Signals', 
                  'Recommendation', 
                  'Processing Time (s)', 
                 ]

        writer.writerow(header)
        
        # Data rows
        for result in batch_result.results:
            # Get top warning signals
            top_signals = [s.name for s in result.signals if s.status.value in ['flagged', 'warning']][:2]
            signals_str = "; ".join(top_signals) if top_signals else "All tests passed"
            
            # Recommendation
            if (result.status == DetectionStatus.REVIEW_REQUIRED):
                recommendation = "Manual verification recommended"

            else:
                recommendation = "No further action needed"
            
            row = [result.filename,
                   f"{result.image_size[0]}×{result.image_size[1]}",
                   result.status.value,
                   f"{result.overall_score:.3f}",
                   f"{result.confidence}%",
                   signals_str,
                   recommendation,
                   f"{result.processing_time:.2f}", 
                  ]
            
            writer.writerow(row)
        
        writer.writerow([])
    

    def _write_detailed_image_section(self, writer, result: AnalysisResult, image_number: int, total_images: int) -> None:
        """
        Write detailed section for single image
        """
        writer.writerow([f'IMAGE {image_number} OF {total_images}'])
        writer.writerow([])
        
        # Basic Information
        writer.writerow(['BASIC INFORMATION'])
        writer.writerow(['Filename', result.filename])
        writer.writerow(['Status', result.status.value])
        writer.writerow(['Overall Score', f"{result.overall_score:.3f}"])
        writer.writerow(['Confidence', f"{result.confidence}%"])
        writer.writerow(['Image Size', f"{result.image_size[0]}×{result.image_size[1]}"])
        writer.writerow(['Processing Time', f"{result.processing_time:.2f}s"])
        writer.writerow(['Timestamp', result.timestamp.isoformat()])
        writer.writerow([])
        
        # Detection Signals
        writer.writerow(['DETECTION SIGNALS'])
        writer.writerow([])
        writer.writerow(['Metric Name', 'Metric Score', 'Analysis Status', 'Metric Confidence', 'Metric Explanation'])
        
        for signal in result.signals:
            metric_result  = result.metric_results.get(signal.metric_type)
            confidence_str = f"{metric_result.confidence:.3f}" if metric_result.confidence is not None else "N/A"
            
            writer.writerow([signal.name,
                             f"{signal.score:.3f}",
                             signal.status.value.upper(),
                             confidence_str,
                             signal.explanation.replace("\n", " "),
                           ])
        
        writer.writerow([])
        
        # Detailed Forensics
        writer.writerow(['FORENSIC DETAILS'])
        writer.writerow([])

        for metric_type in MetricType:
            metric_result = result.metric_results.get(metric_type)
            
            if not metric_result:
                continue

            metric_name = self.detailed_maker.metric_display_names.get(metric_type, metric_type.value)
            
            writer.writerow([f'--- {metric_name} ---'])
            writer.writerow(['Score', f"{metric_result.score:.3f}"])
            writer.writerow(['Confidence', f"{metric_result.confidence:.3f}" if metric_result.confidence is not None else "N/A"])
            
            # Write details
            if metric_result.details:
                for key, value in metric_result.details.items():
                    if isinstance(value, dict):
                        writer.writerow([f"  {key}:", ""])
                        for sub_key, sub_value in value.items():
                            writer.writerow([f"    {sub_key}", str(sub_value)])
                    
                    else:
                        writer.writerow([f"  {key}", str(value)])
            
            writer.writerow([])
        
        # Recommendation
        writer.writerow(['RECOMMENDATION'])
        writer.writerow([])
        
        if (result.status == DetectionStatus.REVIEW_REQUIRED):
            writer.writerow(['Action', 'Manual verification recommended'])
            writer.writerow(['Priority', 'HIGH' if (result.overall_score >= 0.85) else 'MEDIUM'])
            writer.writerow(['Next Steps', 'Forensic analysis, reverse image search, metadata inspection'])
        
        else:
            writer.writerow(['Action', 'No immediate action needed'])
            writer.writerow(['Priority', 'LOW'])
            writer.writerow(['Next Steps', 'Proceed with normal workflow'])
        
        writer.writerow([])
    

    def _write_footer(self, writer) -> None:
        """
        Write CSV report footer
        """
        writer.writerow(['=' * 100])
        writer.writerow(['Report generated by AI Image Screener'])
        writer.writerow(['For questions or support, contact: support@aiimagescreener.com'])
        writer.writerow(['DISCLAIMER: Results are indicative and should be verified manually for critical applications'])
        writer.writerow(['=' * 100])

#############################
# reporter/json_reporter.py
#############################
# Dependencies
import json
from typing import Dict
from typing import List
from pathlib import Path
from typing import Optional
from datetime import datetime
from utils.logger import get_logger
from config.settings import settings
from config.schemas import AnalysisResult
from utils.helpers import generate_unique_id
from config.schemas import BatchAnalysisResult
from features.detailed_result_maker import DetailedResultMaker


# Setup Logging
logger = get_logger(__name__)


class JSONReporter:
    """
    Professional JSON report generator
    
    Features:
    ---------
    - Machine-readable structured format
    - API-friendly output
    - Complete data preservation
    - Pretty-printed for readability
    - Nested structure for complex data
    """
    def __init__(self):
        """
        Initialize JSON Reporter
        """
        self.detailed_maker = DetailedResultMaker()
        logger.debug("JSONReporter initialized")
    

    def export_batch(self, batch_result: BatchAnalysisResult, output_dir: Optional[Path] = None, include_detailed: bool = True) -> Path:
        """
        Export batch analysis as JSON
        
        Arguments:
        ----------
            batch_result     { BatchAnalysisResult } : Complete batch analysis result
            
            output_dir       { Path }                : Output directory (defaults to settings.REPORTS_DIR)
            
            include_detailed { bool }                : Include detailed forensic data
        
        Returns:
        --------
                        { Path }                     : Path to generated JSON file
        """
        output_dir  = output_dir or settings.REPORTS_DIR
        report_id   = generate_unique_id()
        filename    = f"batch_report_{report_id}.json"
        output_path = output_dir / filename

        output_dir.mkdir(parents = True, exist_ok = True)
        
        logger.info(f"Generating batch JSON: {filename}")
        
        try:
            # Build JSON structure
            data = self._build_batch_json(batch_result     = batch_result,
                                          include_detailed = include_detailed,
                                         )
            
            # Write to file
            with open(output_path, 'w', encoding = 'utf-8') as f:
                json.dump(obj          = data, 
                          fp           = f, 
                          indent       = 4, 
                          ensure_ascii = False, 
                          default      = str,
                         )
            
            logger.info(f"Batch JSON generated: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"Failed to generate batch JSON: {e}")
            raise
    

    def export_single(self, result: AnalysisResult, output_dir: Optional[Path] = None, include_detailed: bool = True) -> Path:
        """
        Export single image analysis as JSON
        
        Arguments:
        ----------
            result           { AnalysisResult } : Single image analysis result
            
            output_dir            { Path }      : Output directory (defaults to settings.REPORTS_DIR)
            
            include_detailed      { bool }      : Include detailed forensic data
        
        Returns:
        --------
                      { Path }                  : Path to generated JSON file
        """
        output_dir  = output_dir or settings.REPORTS_DIR
        report_id   = generate_unique_id()
        filename    = f"single_report_{report_id}.json"
        output_path = output_dir / filename

        output_dir.mkdir(parents = True, exist_ok = True)
        
        logger.info(f"Generating single image JSON: {filename}")
        
        try:
            # Build JSON structure
            data = self._build_single_json(result           = result,
                                           include_detailed = include_detailed,
                                          )
            
            # Write to file
            with open(output_path, 'w', encoding = 'utf-8') as f:
                json.dump(obj          = data, 
                          fp           = f, 
                          indent       = 4, 
                          ensure_ascii = False, 
                          default      = str,
                         )
            
            logger.info(f"Single image JSON generated: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"Failed to generate single image JSON: {e}")
            raise
    

    def export_api_response(self, result: AnalysisResult) -> Dict:
        """
        Generate API-friendly JSON response (in-memory, no file)
        
        Arguments:
        ----------
            result { AnalysisResult } : Analysis result
        
        Returns:
        --------
                   { dict }           : API response dictionary
        """
        return {"success"   : True,
                "timestamp" : datetime.now().isoformat(),
                "version"   : settings.VERSION,
                "data"      : self._build_single_json(result           = result,
                                                      include_detailed = False,
                                                     ),
               }
    

    def _build_batch_json(self, batch_result: BatchAnalysisResult, include_detailed: bool) -> Dict:
        """
        Build complete batch JSON structure
        """
        data = {"report_metadata" : self._build_metadata(report_type = "Batch Analysis",
                                                         timestamp   = batch_result.timestamp,
                                                        ),
                "batch_summary"   : self._build_batch_summary(batch_result = batch_result),
                "results"         : [],
               }
        
        # Add each image result
        for result in batch_result.results:
            image_data = self._build_image_data(result           = result,
                                                include_detailed = include_detailed,
                                               )
            data["results"].append(image_data)
        
        return data
    

    def _build_single_json(self, result: AnalysisResult, include_detailed: bool) -> Dict:
        """
        Build single image JSON structure
        """
        data = {"report_metadata" : self._build_metadata(report_type = "Single Image Analysis",
                                                         timestamp   = result.timestamp,
                                                        ),
                "analysis"        : self._build_image_data(result           = result,
                                                           include_detailed = include_detailed,
                                                          ),
               }
        
        return data
    

    def _build_metadata(self, report_type: str, timestamp: datetime) -> Dict:
        """
        Build report metadata section
        """
        return {"report_type"    : report_type,
                "generated_at"   : timestamp.isoformat(),
                "generator"      : "AI Image Screener",
                "version"        : settings.VERSION,
                "format_version" : "1.0",
               }
    

    def _build_batch_summary(self, batch_result: BatchAnalysisResult) -> Dict:
        """
        Build batch summary section
        """
        return {"total_images"          : batch_result.total_images,
                "processed"             : batch_result.processed,
                "failed"                : batch_result.failed,
                "success_rate"          : batch_result.summary.get('success_rate', 0),
                "statistics"            : {"likely_authentic" : batch_result.summary.get('likely_authentic', 0),
                                           "review_required"  : batch_result.summary.get('review_required', 0),
                                           "avg_score"        : batch_result.summary.get('avg_score', 0.0),
                                           "avg_confidence"   : batch_result.summary.get('avg_confidence', 0),
                                           "avg_proc_time"    : batch_result.summary.get('avg_proc_time', 0.0),
                                          },
                "total_processing_time" : round(batch_result.total_processing_time, 2),
               }
    

    def _build_image_data(self, result: AnalysisResult, include_detailed: bool) -> Dict:
        """
        Build complete image data structure
        """
        image_data = {"filename"     : result.filename,
                      "status"       : result.status.value,
                      "overall"      : {"score"           : round(result.overall_score, 3),
                                        "confidence"      : result.confidence,
                                        "interpretation"  : self._interpret_score(score = result.overall_score),
                                       },
                      "image_info"   : {"size"            : {"width"  : result.image_size[0],
                                                             "height" : result.image_size[1],
                                                            },
                                        "processing_time" : round(result.processing_time, 2),
                                        "timestamp"       : result.timestamp.isoformat(),
                                       },
                      "signals"      : self._build_signals_data(result = result),
                     }
        
        # Add detailed forensics if requested
        if include_detailed:
            image_data["forensics"]       = self._build_forensics_data(result = result)
            image_data["recommendations"] = self._build_recommendations(result = result)
        
        return image_data
    

    def _build_signals_data(self, result: AnalysisResult) -> List[Dict]:
        """
        Build signals data structure
        """
        signals = list()
        
        for signal in result.signals:
            metric_result = result.metric_results.get(signal.metric_type)
            
            signal_data   = {"metric_name" : signal.name,
                             "metric_type" : signal.metric_type.value,
                             "score"       : round(signal.score, 3),
                             "status"      : signal.status.value,
                             "confidence"  : round(metric_result.confidence, 3) if (metric_result and metric_result.confidence is not None) else None,
                             "explanation" : signal.explanation,
                            }
            
            signals.append(signal_data)
        
        return signals
    

    def _build_forensics_data(self, result: AnalysisResult) -> Dict:
        """
        Build detailed forensics data structure
        """
        forensics = dict()
        
        for metric_type, metric_result in result.metric_results.items():
            metric_name                  = self.detailed_maker.metric_display_names.get(metric_type, metric_type.value)
            
            forensics[metric_type.value] = {"display_name" : metric_name,
                                            "score"        : round(metric_result.score, 3),
                                            "confidence"   : round(metric_result.confidence, 3) if (metric_result and metric_result.confidence is not None) else None,
                                            "details"      : metric_result.details or {},
                                            "key_findings" : self.detailed_maker.extract_key_findings(metric_type   = metric_type,
                                                                                                      metric_result = metric_result,
                                                                                                     ),
                                           }
        
        return forensics
    

    def _build_recommendations(self, result: AnalysisResult) -> Dict:
        """
        Build recommendations structure
        """
        score = result.overall_score
        
        if (score >= 0.85):
            return {"action"      : "Immediate manual verification required",
                    "priority"    : "HIGH",
                    "risk_level"  : "CRITICAL",
                    "next_steps"  : ["Forensic analysis", "Reverse image search", "Metadata inspection"],
                    "confidence"  : "Very high likelihood of AI generation",
                   }
        
        elif (score >= 0.70):
            return {"action"      : "Manual verification recommended",
                    "priority"    : "MEDIUM",
                    "risk_level"  : "HIGH",
                    "next_steps"  : ["Visual inspection", "Compare with authentic samples"],
                    "confidence"  : "High likelihood of AI generation",
                   }
        
        elif (score >= 0.50):
            return {"action"      : "Optional review suggested",
                    "priority"    : "LOW",
                    "risk_level"  : "MEDIUM",
                    "next_steps"  : ["Verify image source", "Check for inconsistencies"],
                    "confidence"  : "Moderate indicators present",
                   }
        
        else:
            return {"action"      : "No immediate action required",
                    "priority"    : "NONE",
                    "risk_level"  : "LOW",
                    "next_steps"  : ["Proceed with normal workflow"],
                    "confidence"  : "Low likelihood of AI generation",
                   }
    

    def _interpret_score(self, score: float) -> str:
        """
        Interpret score for human readability
        """
        if (score >= 0.85):
            return "Very high suspicion"

        elif (score >= 0.70):
            return "High suspicion"
        
        elif (score >= 0.50):
            return "Moderate suspicion"
        
        elif (score >= 0.30):
            return "Low suspicion"
        
        else:
            return "Very low suspicion"

#############################
# reporter/pdf_reporter.py
#############################
# Dependencies
from pathlib import Path
from typing import Optional
from datetime import datetime
from utils.logger import get_logger
from config.settings import settings
from reportlab.platypus import Table
from reportlab.platypus import Spacer
from reportlab.lib.colors import grey
from reportlab.lib.colors import black
from reportlab.lib.pagesizes import A4
from reportlab.lib.enums import TA_LEFT
from reportlab.platypus import Paragraph
from reportlab.platypus import PageBreak
from reportlab.lib.enums import TA_CENTER
from reportlab.platypus import TableStyle
from config.schemas import AnalysisResult
from utils.helpers import generate_unique_id
from config.constants import DetectionStatus
from config.schemas import BatchAnalysisResult
from reportlab.lib.styles import ParagraphStyle
from reportlab.platypus import SimpleDocTemplate
from reportlab.lib.styles import getSampleStyleSheet
from features.detailed_result_maker import DetailedResultMaker


# Setup Logging
logger = get_logger(__name__)


class PDFReporter:
    """
    Professional PDF report generator using ReportLab

    Features:
    ---------
    - Single image forensic PDF
    - Batch PDF with per-image sections
    - Clear visual hierarchy
    - Human-readable explanations
    - Reuses DetailedResultMaker (no recomputation)
    """
    def __init__(self):
        self.detailed_maker = DetailedResultMaker()
        self.styles         = self._build_styles()

        logger.debug("PDFReporter initialized")


    def export_single(self, result: AnalysisResult, output_dir: Optional[Path] = None) -> Path:
        """
        Export a single image analysis PDF
        """
        output_dir  = output_dir or settings.REPORTS_DIR
        output_dir.mkdir(parents = True, exist_ok = True)

        report_id   = generate_unique_id()
        filename    = f"single_analysis_{report_id}.pdf"
        output_path = output_dir / filename

        logger.info(f"Generating single PDF: {filename}")

        doc         = self._create_document(path = output_path)
        story       = list()

        self._add_report_header(story     = story, 
                                title     = "Single Image Analysis", 
                                timestamp = result.timestamp,
                               )

        self._add_image_section(story  = story, 
                                result = result,
                               )

        doc.build(story)

        return output_path


    def export_batch(self, batch_result: BatchAnalysisResult, output_dir: Optional[Path] = None) -> Path:
        """
        Export a batch analysis PDF
        """
        output_dir  = output_dir or settings.REPORTS_DIR
        output_dir.mkdir(parents = True, exist_ok = True)

        report_id   = generate_unique_id()
        filename    = f"batch_analysis_{report_id}.pdf"
        output_path = output_dir / filename

        logger.info(f"Generating batch PDF: {filename}")

        doc         = self._create_document(path = output_path)
        story       = list()

        self._add_report_header(story     = story, 
                                title     = "Batch Image Analysis", 
                                timestamp = batch_result.timestamp,
                               )

        self._add_batch_summary(story        = story, 
                                batch_result = batch_result,
                               )

        for idx, result in enumerate(batch_result.results, 1):
            story.append(PageBreak())
            self._add_image_section(story  = story, 
                                    result = result, 
                                    index  = idx, 
                                    total  = batch_result.processed,
                                   )

        doc.build(story)

        return output_path


    def _create_document(self, path: Path) -> SimpleDocTemplate:
        return SimpleDocTemplate(str(path),
                                 pagesize     = A4,
                                 rightMargin  = 25,
                                 leftMargin   = 25,
                                 topMargin    = 25,
                                 bottomMargin = 25,
                                )


    def _build_styles(self):
        styles = getSampleStyleSheet()

        styles.add(ParagraphStyle(name       = "CustomTitle",
                                  fontSize   = 18,
                                  alignment  = TA_CENTER,
                                  spaceAfter = 12,
                                 )
                  )

        styles.add(ParagraphStyle(name        = "Section",
                                  fontSize    = 14,
                                  spaceBefore = 14,
                                  spaceAfter  = 8,
                                  textColor   = black,
                                 )
                  )

        styles.add(ParagraphStyle(name       = "Body",
                                  fontSize   = 10,
                                  spaceAfter = 6,
                                  alignment  = TA_LEFT,
                                 )
                  )

        styles.add(ParagraphStyle(name      = "Muted",
                                  fontSize  = 9,
                                  textColor = grey,
                                 )
                  )

        return styles


    def _add_report_header(self, story, title: str, timestamp: datetime):
        story.append(Paragraph("AI Image Screener", self.styles["Title"]))
        story.append(Paragraph(title, self.styles["Section"]))
        
        story.append(Paragraph(f"Generated: {timestamp.strftime('%Y-%m-%d %H:%M:%S')} | Version: {settings.VERSION}",
                               self.styles["Muted"],
                              )
                    )

        story.append(Spacer(1, 16))


    def _add_batch_summary(self, story, batch_result: BatchAnalysisResult):
        story.append(Paragraph("Batch Summary", self.styles["Section"]))

        data = [["Total Images", batch_result.total_images],
                ["Processed", batch_result.processed],
                ["Failed", batch_result.failed],
                ["Success Rate", f"{batch_result.summary.get('success_rate', 0)}%"],
                ["Likely Authentic", batch_result.summary.get("likely_authentic", 0)],
                ["Review Required", batch_result.summary.get("review_required", 0)],
                ["Average Score", batch_result.summary.get("avg_score", 0)],
                ["Avg Processing Time", f"{batch_result.summary.get('avg_proc_time', 0)}s"],
               ]

        self._add_kv_table(story, data)


    def _add_image_section(self, story, result: AnalysisResult, index: Optional[int] = None, total: Optional[int] = None):
        title = f"Image Analysis"
        
        if index is not None:
            title += f" ({index}/{total})"

        story.append(Paragraph(title, self.styles["Section"]))

        # Basic info
        info = [["Filename", result.filename],
                ["Status", result.status.value],
                ["Overall Score", f"{result.overall_score:.3f}"],
                ["Confidence", f"{result.confidence}%" if result.confidence is not None else "N/A"],
                ["Image Size", f"{result.image_size[0]} × {result.image_size[1]}"],
                ["Processing Time", f"{result.processing_time:.2f}s"],
               ]

        self._add_kv_table(story, info)

        # Signals
        story.append(Spacer(1, 8))
        story.append(Paragraph("Detection Signals", self.styles["Section"]))

        signal_rows = [["Metric", "Score", "Status", "Explanation"]]
        for s in result.signals:
            signal_rows.append([s.name,
                                f"{s.score:.3f}",
                                s.status.value.upper(),
                                s.explanation,
                              ])

        self._add_table(story, signal_rows)

        # Forensics
        story.append(Spacer(1, 8))
        story.append(Paragraph("Forensic Details", self.styles["Section"]))

        for metric_type, metric_result in result.metric_results.items():
            story.append(Spacer(1, 4))

            metric_name = self.detailed_maker.metric_display_names.get(metric_type, metric_type.value)

            story.append(Paragraph(metric_name, self.styles["Body"]))

            rows        = [["Score", f"{metric_result.score:.3f}"],
                           ["Confidence", metric_result.confidence if metric_result.confidence is not None else "N/A"],
                          ]

            self._add_kv_table(story, rows)

            findings = self.detailed_maker.extract_key_findings(metric_type, metric_result)

            for f in findings:
                story.append(Paragraph(f"• {f}", self.styles["Muted"]))

            story.append(Spacer(1, 6))

        # Recommendation
        story.append(Spacer(1, 8))
        story.append(Paragraph("Recommendation", self.styles["Section"]))

        if (result.status == DetectionStatus.REVIEW_REQUIRED):
            rec = "Manual verification recommended"

        else:
            rec = "No immediate action required"

        story.append(Paragraph(rec, self.styles["Body"]))


    def _add_kv_table(self, story, rows):
        table = Table(rows, colWidths = [160, 320])
        
        table.setStyle(TableStyle([("GRID", (0, 0), (-1, -1), 0.25, grey),
                                   ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                                 ])
                      )

        story.append(table)
        story.append(Spacer(1, 8))


    def _add_table(self, story, rows):
        table = Table(rows, repeatRows=1)
        
        table.setStyle(TableStyle([("GRID", (0, 0), (-1, -1), 0.25, grey),
                                   ("BACKGROUND", (0, 0), (-1, 0), grey),
                                   ("TEXTCOLOR", (0, 0), (-1, 0), black),
                                   ("FONTNAME", (0, 0), (-1, 0), "Helvetica-Bold"),
                                   ("VALIGN", (0, 0), (-1, -1), "TOP"),
                                 ])
                      )

        story.append(table)
        story.append(Spacer(1, 8))


################################
# app.py
################################
## Dependencies
import uuid
import shutil
import signal
import uvicorn
import traceback
from typing import List
from typing import Dict
from pathlib import Path
from fastapi import File
from typing import Optional
from fastapi import Request
from fastapi import FastAPI
from fastapi import UploadFile
from fastapi import HTTPException
from utils.logger import get_logger
from config.settings import settings
from fastapi.responses import Response
from config.schemas import APIResponse
from config.schemas import AnalysisResult
from fastapi.responses import HTMLResponse
from fastapi.responses import JSONResponse
from utils.validators import ImageValidator
from fastapi.staticfiles import StaticFiles
from utils.helpers import generate_unique_id
from reporter.csv_reporter import CSVReporter
from reporter.pdf_reporter import PDFReporter
from config.schemas import BatchAnalysisResult
from reporter.json_reporter import JSONReporter
from utils.image_processor import ImageProcessor
from fastapi.middleware.cors import CORSMiddleware
from features.batch_processor import BatchProcessor
from features.threshold_manager import ThresholdManager


# Logging
logger = get_logger(__name__)


# FastAPI App Definition
app = FastAPI(title       = "AI Image Screener",
              version     = settings.VERSION,
              description = "First-pass AI image screening tool for bulk workflows",
             )


# Serve static assets (if any later)
app.mount("/ui", StaticFiles(directory = "ui"), name = "ui")

# CORS (UI + API)
app.add_middleware(CORSMiddleware,
                   allow_origins     = ["*"],
                   allow_credentials = True,
                   allow_methods     = ["*"],
                   allow_headers     = ["*"],
                  )

# Runtime State
SESSION_STORE: Dict[str, Dict] = {}

# Component Initialization
image_validator   = ImageValidator()
image_processor   = ImageProcessor()

threshold_manager = ThresholdManager()
threshold_manager = threshold_manager
batch_processor   = BatchProcessor(threshold_manager = threshold_manager)

json_reporter     = JSONReporter()
csv_reporter      = CSVReporter()
pdf_reporter      = PDFReporter()

UPLOAD_DIR        = settings.UPLOAD_DIR
CACHE_DIR         = settings.CACHE_DIR
REPORTS_DIR       = settings.REPORTS_DIR

for d in [UPLOAD_DIR, CACHE_DIR, REPORTS_DIR]:
    d.mkdir(parents  = True, 
            exist_ok = True,
           )


# Utility: Progress Callback
def _progress_callback(batch_id: str):
    def callback(image_idx: int, total: int, filename: str):
        session = SESSION_STORE.get(batch_id)
        if (not session or (session.get("status") != "processing")):
            return

        session["progress"] = {"current"  : image_idx,
                               "total"    : total,
                               "filename" : filename,
                              }
    return callback


# Utility: Housekeeping
def cleanup_temp_files():
    try:
        for folder in [UPLOAD_DIR, CACHE_DIR]:
            for item in folder.iterdir():
                if item.is_file():
                    item.unlink(missing_ok = True)

        logger.info("Temporary files cleaned")

    except Exception as e:
        logger.warning(f"Cleanup failed: {e}")


def shutdown_handler(*_):
    logger.warning("Shutdown signal received — cleaning up")
    cleanup_temp_files()


signal.signal(signal.SIGINT, shutdown_handler)
signal.signal(signal.SIGTERM, shutdown_handler)


# Error Handling
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unhandled error: {exc}")
    logger.debug(traceback.format_exc())
    
    return JSONResponse(status_code = 500,
                        content     = APIResponse(success = False,
                                                  message = "Internal server error",
                                                 ).model_dump()
                       )


# Home
@app.get("/", response_class = HTMLResponse)
def serve_frontend():
    index_path = Path("ui/index.html")

    if not index_path.exists():
        raise HTTPException(status_code = 404,
                            detail      = "UI not found",
                           )

    return index_path.read_text(encoding = "utf-8")


# Health Check
@app.get("/health")
def health():
    return {"status"  : "ok",
            "version" : settings.VERSION,
           }


# Single Image Analysis
@app.post("/analyze/image")
async def analyze_single_image(file: UploadFile = File(...)):
    image_id   = generate_unique_id()
    image_path = UPLOAD_DIR / f"{image_id}_{file.filename}"

    image_validator.validate_image(file_path = image_path,
                                   filename  = file.filename,
                                   file_size = file.size,
                                  )

    try:
        with open(image_path, "wb") as f:
            shutil.copyfileobj(file.file, f)

        image                  = image_processor.load_image(image_path)

        # image is a NumPy array → shape = (H, W, C) or (H, W)
        height, width          = image.shape[:2]

        result: AnalysisResult = batch_processor.process_single(image      = image_path,
                                                                filename   = file.filename,
                                                                image_size = (width, height),
                                                               )

        return APIResponse(success = True,
                           message = "Image analysis completed",
                           data    = result.model_dump(),
                          )

    finally:
        image_path.unlink(missing_ok = True)


# Batch Image Analysis
@app.post("/analyze/batch")
async def analyze_batch(files: List[UploadFile] = File(...)):
    if not files:
        raise HTTPException(status_code = 400, 
                            detail      = "No files provided",
                           )

    batch_id                = str(uuid.uuid4())

    SESSION_STORE[batch_id] = {"status"   : "processing",
                               "progress" : {"current" : 0, 
                                             "total"   : len(files),
                                            },
                              }

    image_entries           = list()

    try:
        for file in files:
            uid           = generate_unique_id()
            path          = UPLOAD_DIR / f"{uid}_{file.filename}"

            with open(path, "wb") as f:
                shutil.copyfileobj(file.file, f)
            
            image         = image_processor.load_image(path)
            height, width = image.shape[:2]

            image_validator.validate_image(file_path = path,
                                           filename  = file.filename,
                                           file_size = file.size,
                                          )

            image_entries.append({"path"     : path,
                                  "filename" : file.filename,
                                  "size"     : (width, height),
                                })

        batch_result: BatchAnalysisResult = batch_processor.process_batch(image_files = image_entries,
                                                                          on_progress = _progress_callback(batch_id),
                                                                         )

        SESSION_STORE[batch_id]           = {"status"   : "completed",
                                             "progress" : SESSION_STORE[batch_id]["progress"],
                                             "result"   : batch_result,       
                                            }

        
        return APIResponse(success = True,
                           message = "Batch analysis completed",
                           data    = {"batch_id" : batch_id,
                                      "result"   : batch_result.model_dump(),
                                     },
                          )

    except KeyboardInterrupt:
        SESSION_STORE[batch_id] = {"status"   : "interrupted",
                                   "progress" : SESSION_STORE[batch_id]["progress"],
                                  }

        raise HTTPException(status_code = 499, 
                            detail      = "Processing interrupted",
                           )

    except Exception as e:
        logger.error(f"Batch {batch_id} failed: {e}", exc_info = True)
        
        SESSION_STORE[batch_id] = {"status" : "failed",
                                   "error"  : str(e),
                                  }

        raise HTTPException(status_code = 500, 
                            detail      = "Batch processing failed",
                           )

    finally:
        for item in image_entries:
            Path(item["path"]).unlink(missing_ok = True)


# Batch Progress
@app.get("/batch/{batch_id}/progress")
def batch_progress(batch_id: str):
    session = SESSION_STORE.get(batch_id)
    
    if not session:
        raise HTTPException(status_code = 404,
                            detail      = "Batch not found",
                           )
    
    return session


# Report Downloads
@app.api_route("/report/csv/{batch_id}", methods = ["GET", "POST"])
def export_csv(batch_id: str):
    session = SESSION_STORE.get(batch_id)

    if (not session or ("result" not in session)):
        raise HTTPException(status_code = 404, 
                            detail      = "Batch result not found",
                           )

    path = csv_reporter.export_batch_detailed(session["result"])
    
    # Read the file and send it as a download
    with open(path, "rb") as f:
        content = f.read()
    
    # Clean up the file after sending
    path.unlink(missing_ok = True)
    
    return Response(content    = content,
                    media_type = "text/csv",
                    headers    = {"Content-Disposition" : f"attachment; filename=ai_screener_report_{batch_id}.csv",
                                  "Content-Type"        : "text/csv"
                                 }
                   )


@app.api_route("/report/pdf/{batch_id}", methods = ["GET", "POST"])
def export_pdf(batch_id: str):
    session = SESSION_STORE.get(batch_id)

    if (not session or ("result" not in session)):
        raise HTTPException(status_code = 404, 
                            detail      = "Batch result not found",
                           )

    path = pdf_reporter.export_batch(session["result"])
    
    # Read the file and send it as a download
    with open(path, "rb") as f:
        content = f.read()
    
    # Clean up the file after sending
    path.unlink(missing_ok = True)
    
    return Response(content    = content,
                    media_type = "application/pdf",
                    headers    = {"Content-Disposition" : f"attachment; filename=ai_screener_report_{batch_id}.pdf",
                                  "Content-Type"        : "application/pdf"
                                 }
                   )



# ==================== MAIN ====================
if __name__ == "__main__":
    # Explicit startup log (forces log file creation)
    logger.info("Starting AI Image Screener API Server")

    uvicorn.run("app:app",                    
                host      = settings.HOST,
                port      = settings.PORT,
                reload    = settings.DEBUG,
                log_level = settings.LOG_LEVEL.lower(),
                workers   = 1 if settings.DEBUG else settings.WORKERS,
               )