{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d654dc-c431-420e-810a-a985de9172fd",
   "metadata": {},
   "source": [
    "# Unified AI vs Real Image Dataset Builder\n",
    "\n",
    "This notebook builds a **clean, labeled, unified dataset** for evaluating\n",
    "AI image detection systems.\n",
    "\n",
    "### Supported sources\n",
    "- HuggingFace datasets (DiffusionDB, COCO, OpenImages)\n",
    "- Kaggle public datasets (Midjourney, AI vs Real)\n",
    "- Unified output format:\n",
    "  - Normalized PNG images\n",
    "  - Size-limited (â‰¤1024px)\n",
    "  - Central metadata CSV\n",
    "\n",
    "### Output Structure\n",
    "\n",
    "```bash\n",
    "tests/dataset/\n",
    "â”œâ”€â”€ ai/\n",
    "â”œâ”€â”€ real/\n",
    "â”œâ”€â”€ raw_downloads/\n",
    "â”œâ”€â”€ metadata/dataset_index.csv\n",
    "```\n",
    "\n",
    "> âš ï¸ All datasets used are **public & legally accessible**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b43897-9ce5-4f20-8798-7b3aebdf1b36",
   "metadata": {},
   "source": [
    "## Required Dependencies\n",
    "\n",
    "Before running, ensure:\n",
    "\n",
    "```bash\n",
    "pip install datasets pillow tqdm kaggle pycocotools\n",
    "```\n",
    "\n",
    "Also configure Kaggle:\n",
    "\n",
    "```bash\n",
    "~/.kaggle/kaggle.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9f50c-6158-47e9-89cf-5c279d9c63bb",
   "metadata": {},
   "source": [
    "## Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9147ace7-162f-4b0d-bd6d-0d92b9bad61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Imports & Global Configuration\n",
    "# ===============================\n",
    "import os\n",
    "import csv\n",
    "import uuid\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Directory Configuration\n",
    "# ===============================\n",
    "BASE_DIR       = Path(\"../tests/dataset\")\n",
    "AI_DIR         = BASE_DIR / \"ai\"\n",
    "REAL_DIR       = BASE_DIR / \"real\"\n",
    "RAW_DIR        = BASE_DIR / \"raw_downloads\"\n",
    "META_DIR       = BASE_DIR / \"metadata\"\n",
    "\n",
    "META_FILE      = META_DIR / \"dataset_index.csv\"\n",
    "\n",
    "TARGET_PER_DS  = 1000\n",
    "IMAGE_SIZE_MAX = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d1c09-0e9c-4bc2-8935-bd50941611c8",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "These helpers:\n",
    "- Ensure directory structure\n",
    "- Normalize images (RGB, resize, PNG)\n",
    "- Write metadata rows safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b352e981-e456-40cf-be84-a1eb0f01ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dirs():\n",
    "    for d in [AI_DIR, REAL_DIR, RAW_DIR, META_DIR]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def normalize_and_save(image: Image.Image, path: Path):\n",
    "    \"\"\"\n",
    "    Normalize image to RGB PNG and limit size\n",
    "    \"\"\"\n",
    "    image = image.convert(\"RGB\")\n",
    "    image.thumbnail((IMAGE_SIZE_MAX, IMAGE_SIZE_MAX))\n",
    "    image.save(path, \n",
    "               format   = \"PNG\", \n",
    "               optimize = True,\n",
    "              )\n",
    "\n",
    "\n",
    "def write_meta(writer, **row):\n",
    "    writer.writerow(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3bc3b-6bb6-414d-b3fe-85bc43d832c7",
   "metadata": {},
   "source": [
    "## Dataset Registry\n",
    "\n",
    "Defines **where data comes from** and **how it is labeled**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74106705-e2d6-411c-8193-8e02f5ee0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace datasets (safe & stable)\n",
    "AI_DATASETS     = [{\"name\"      : \"diffusiondb\",\n",
    "                    \"hf_id\"     : \"poloclub/diffusiondb\",\n",
    "                    \"config\"    : \"2m_first_1k\",\n",
    "                    \"split\"     : \"train\",\n",
    "                    \"image_key\" : \"image\",\n",
    "                    \"label\"     : \"ai\",\n",
    "                    \"family\"    : \"diffusion\"\n",
    "                  }]\n",
    "        \n",
    "\n",
    "REAL_DATASETS   = [{\"name\"      : \"imagenette\",\n",
    "                    \"hf_id\"     : \"frgfm/imagenette\",\n",
    "                    \"config\"    : \"320px\",\n",
    "                    \"split\"     : \"train\",\n",
    "                    \"image_key\" : \"image\",\n",
    "                    \"label\"     : \"real\",\n",
    "                    \"family\"    : \"photographic\",\n",
    "                  }]\n",
    "\n",
    "# Kaggle datasets (public, non-scraped)\n",
    "KAGGLE_DATASETS = [{\"name\"      : \"ai_vs_real\",\n",
    "                    \"kaggle_id\" : \"tristanzhang32/ai-generated-images-vs-real-images\",\n",
    "                    \"label\"     : \"ai\",\n",
    "                    \"family\"    : \"mixed\"\n",
    "                   },\n",
    "                   {\"name\"      : \"midjourney\",\n",
    "                    \"kaggle_id\" : \"cyanex1702/midjourney-imagesprompt\",\n",
    "                    \"label\"     : \"ai\",\n",
    "                    \"family\"    : \"diffusion\"\n",
    "                   }\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c6f3b-2a35-415b-9a35-ee52fd3d85be",
   "metadata": {},
   "source": [
    "## HuggingFace Dataset Processor\n",
    "\n",
    "Loads datasets via `datasets.load_dataset()` and saves images in unified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ea5276-65bb-49f5-a656-c00ceeb1f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hf_dataset(ds_cfg, root_dir, writer):\n",
    "    print(f\"\\nâ–¶ Loading HF dataset: {ds_cfg['name']}\")\n",
    "\n",
    "    ds      = load_dataset(ds_cfg[\"hf_id\"],\n",
    "                           **ds_cfg.get(\"hf_kwargs\", {}),\n",
    "                           name      = ds_cfg.get(\"config\"),\n",
    "                           split     = ds_cfg[\"split\"],\n",
    "                           streaming = ds_cfg.get(\"streaming\", False),\n",
    "                          )\n",
    "\n",
    "    out_dir = root_dir / ds_cfg[\"name\"]\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    count   = 0\n",
    "    \n",
    "    for row in tqdm(ds):\n",
    "        if (count >= TARGET_PER_DS):\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            image = row.get(ds_cfg[\"image_key\"])\n",
    "            if not isinstance(image, Image.Image):\n",
    "                continue\n",
    "\n",
    "            uid   = uuid.uuid4().hex\n",
    "            path  = out_dir / f\"{uid}.png\"\n",
    "\n",
    "            normalize_and_save(image, path)\n",
    "\n",
    "            write_meta(writer,\n",
    "                       id       = uid,\n",
    "                       filename = str(path),\n",
    "                       label    = ds_cfg[\"label\"],\n",
    "                       family   = ds_cfg[\"family\"],\n",
    "                       source   = ds_cfg[\"name\"],\n",
    "                      )\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d23a0-fa98-4351-9e4e-99265a51e8ef",
   "metadata": {},
   "source": [
    "## Kaggle Dataset Downloader\n",
    "\n",
    "Requires:\n",
    "- Kaggle account\n",
    "- ~/.kaggle/kaggle.json configured\n",
    "\n",
    "No scraping. Fully legal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6eca5e6-0469-4af6-8af8-afe3036cb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_dataset(kaggle_id: str, out_dir: Path):\n",
    "    out_dir.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    if any(out_dir.iterdir()):\n",
    "        print(f\"âœ” Kaggle dataset already present: {kaggle_id}\")\n",
    "        return\n",
    "\n",
    "    print(f\"â¬‡ Downloading Kaggle dataset: {kaggle_id}\")\n",
    "\n",
    "    subprocess.run([\"kaggle\", \"datasets\", \"download\",\n",
    "                    kaggle_id,\n",
    "                    \"-p\", str(out_dir),\n",
    "                    \"--unzip\"\n",
    "                   ],\n",
    "                   check = True,\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c971767-d20a-4fa3-949a-a655d712b2c1",
   "metadata": {},
   "source": [
    "## Folder Ingestor\n",
    "\n",
    "Converts **any folder of images** into the unified dataset format. \n",
    "Used for Kaggle & future web sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b648832e-5025-4851-af21-382051167a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_EXTS = {\".png\", \".jpg\", \".jpeg\", \".webp\"}\n",
    "\n",
    "def ingest_image_folder(src_dir, out_dir, writer, label, family, source):\n",
    "    images = [p for p in src_dir.rglob(\"*\") if p.suffix.lower() in IMAGE_EXTS]\n",
    "\n",
    "    out_dir.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    for image_path in tqdm(images[:TARGET_PER_DS]):\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            uid   = uuid.uuid4().hex\n",
    "            dst   = out_dir / f\"{uid}.png\"\n",
    "\n",
    "            normalize_and_save(image, dst)\n",
    "\n",
    "            write_meta(writer,\n",
    "                       id       = uid,\n",
    "                       filename = str(dst),\n",
    "                       label    = label,\n",
    "                       family   = family,\n",
    "                       source   = source,\n",
    "                      )\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fccdc4-e593-4dbf-a71b-e5b826e4a27a",
   "metadata": {},
   "source": [
    "## Main Pipeline Execution\n",
    "\n",
    "This cell:\n",
    "- Builds directories\n",
    "- Processes HF datasets\n",
    "- Downloads & ingests Kaggle datasets\n",
    "- Writes unified metadata CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8ef771-f39f-4d9d-8eaf-626ecc211141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â–¶ Loading HF dataset: diffusiondb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [05:31<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â–¶ Loading HF dataset: imagenette\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e436d2fc4374bff9d76dc2534b752b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791d872b01a44cbb908ddbad43f20a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b6ebafe8e4436e9cc3a4bf38a36bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['full_size', '320px', '160px']\nExample of usage:\n\t`load_dataset('imagenette', 'full_size')`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Entry Point\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     process_hf_dataset(ds, AI_DIR, writer)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m REAL_DATASETS:\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mprocess_hf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mREAL_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Kaggle datasets\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m KAGGLE_DATASETS:\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mprocess_hf_dataset\u001b[0;34m(ds_cfg, root_dir, writer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_hf_dataset\u001b[39m(ds_cfg, root_dir, writer):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâ–¶ Loading HF dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     ds      \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mds_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mname\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mds_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                           \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mds_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mds_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstreaming\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     out_dir \u001b[38;5;241m=\u001b[39m root_dir \u001b[38;5;241m/\u001b[39m ds_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m     out_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/mvp_env/lib/python3.10/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/.conda/envs/mvp_env/lib/python3.10/site-packages/datasets/load.py:1852\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1850\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1852\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/.conda/envs/mvp_env/lib/python3.10/site-packages/datasets/builder.py:373\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mvp_env/lib/python3.10/site-packages/datasets/builder.py:525\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    524\u001b[0m     example_of_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_dataset(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig name is missing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease pick one among the available configs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample of usage:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_of_usage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    531\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo config specified, defaulting to the single config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuilder_config\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Config name is missing.\nPlease pick one among the available configs: ['full_size', '320px', '160px']\nExample of usage:\n\t`load_dataset('imagenette', 'full_size')`"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ensure_dirs()\n",
    "\n",
    "    with open(META_FILE, \"w\", newline = \"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"id\", \"filename\", \"label\", \"family\", \"source\"])\n",
    "        writer.writeheader()\n",
    "\n",
    "        # HuggingFace datasets\n",
    "        for ds in AI_DATASETS:\n",
    "            process_hf_dataset(ds, AI_DIR, writer)\n",
    "\n",
    "        for ds in REAL_DATASETS:\n",
    "            process_hf_dataset(ds, REAL_DIR, writer)\n",
    "\n",
    "        # Kaggle datasets\n",
    "        for ds in KAGGLE_DATASETS:\n",
    "            raw_path = RAW_DIR / ds[\"name\"]\n",
    "            download_kaggle_dataset(ds[\"kaggle_id\"], raw_path)\n",
    "\n",
    "            # AI images\n",
    "            ingest_image_folder(src_dir = raw_path / \"ai\",\n",
    "                                out_dir = AI_DIR / ds[\"name\"],\n",
    "                                writer  = writer,\n",
    "                                label   = \"ai\",\n",
    "                                family  = ds[\"family\"],\n",
    "                                source  = ds[\"name\"],\n",
    "                               )\n",
    "\n",
    "            # REAL images\n",
    "            ingest_image_folder(src_dir = raw_path / \"real\",\n",
    "                                out_dir = REAL_DIR / ds[\"name\"],\n",
    "                                writer  = writer,\n",
    "                                label   = \"real\",\n",
    "                                family  = \"photographic\",\n",
    "                                source  = ds[\"name\"],\n",
    "                               )\n",
    "\n",
    "    print(\"\\nâœ… Dataset build complete\")\n",
    "    print(f\"ðŸ“„ Metadata saved at: {META_FILE}\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Entry Point\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e0834-7757-4daf-a8bc-37d58bc8debd",
   "metadata": {},
   "source": [
    "# Post-Processing Attack Generator\n",
    "\n",
    "This notebook applies **real-world post-processing attacks** to an existing\n",
    "image dataset to evaluate robustness of AI-image detectors.\n",
    "\n",
    "### Attacks Implemented\n",
    "- JPEG recompression (quality loss)\n",
    "- Resize / rescale (down + up)\n",
    "- Gaussian blur\n",
    "\n",
    "### Why this matters\n",
    "Most AI images in the wild are:\n",
    "- Screenshot\n",
    "- Re-encoded\n",
    "- Uploaded to social media\n",
    "- Slightly blurred or resized\n",
    "\n",
    "If a detector fails here, it fails in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd680866-0f5c-4930-9262-5521317044fd",
   "metadata": {},
   "source": [
    "## Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62168b8-aa38-47c6-8a00-0bb31e8774fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Imports\n",
    "# ===============================\n",
    "\n",
    "import csv\n",
    "import uuid\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import ImageFilter\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Configuration\n",
    "# ===============================\n",
    "\n",
    "BASE_DIR       = Path(\"tests/dataset\")\n",
    "ATTACK_DIR     = BASE_DIR / \"attacked\"\n",
    "META_IN        = BASE_DIR / \"metadata/dataset_index.csv\"\n",
    "META_OUT       = BASE_DIR / \"metadata/dataset_index_attacked.csv\"\n",
    "\n",
    "ATTACK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "JPEG_QUALITIES = [95, 75, 50]\n",
    "RESIZE_SCALES  = [0.75, 0.5]\n",
    "BLUR_RADII     = [0.8, 1.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1de132-8245-42c7-9a82-63d6f0c27270",
   "metadata": {},
   "source": [
    "## Load Existing Metadata\n",
    "\n",
    "We read the existing unified dataset index and create\n",
    "new samples **derived from originals**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e5629-ba32-4736-b0ab-e81084f58b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(path):\n",
    "    with open(path, newline=\"\") as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "\n",
    "records = load_metadata(META_IN)\n",
    "print(f\"Loaded {len(records)} original samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0e31a-abdf-4564-8696-90aef3fc5ec4",
   "metadata": {},
   "source": [
    "## Attack Primitives\n",
    "\n",
    "Each function:\n",
    "- Takes a PIL Image\n",
    "- Returns a new PIL Image\n",
    "- Does **not** modify the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6027902-897a-4a3b-a806-e715fea43050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jpeg_attack(image: Image.Image, quality: int) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Simulate JPEG recompression\n",
    "    \"\"\"\n",
    "    buf = BytesIO()\n",
    "    image.save(buf, \n",
    "               format  = \"JPEG\", \n",
    "               quality = quality,\n",
    "              )\n",
    "    \n",
    "    buf.seek(0)\n",
    "    return Image.open(buf).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def resize_attack(image: Image.Image, scale: float) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Downscale and upscale image\n",
    "    \"\"\"\n",
    "    w, h         = image.size\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    image_small  = image.resize((new_w, new_h), Image.BICUBIC)\n",
    "    \n",
    "    return image_small.resize((w, h), Image.BICUBIC)\n",
    "\n",
    "\n",
    "def blur_attack(image: Image.Image, radius: float) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Apply Gaussian blur\n",
    "    \"\"\"\n",
    "    return image.filter(ImageFilter.GaussianBlur(radius))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d3ca44-b497-4397-bd35-04db9041d1e4",
   "metadata": {},
   "source": [
    "## Attack Application Pipeline\n",
    "\n",
    "For each original image:\n",
    "- Apply all attack variants\n",
    "- Save attacked images\n",
    "- Write **attack-aware metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702ab79-68b1-4191-8e87-f26ad0227348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_attacks(records, writer):\n",
    "    for r in tqdm(records):\n",
    "        src_path = Path(r[\"filename\"])\n",
    "        \n",
    "        if not src_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img = Image.open(src_path).convert(\"RGB\")\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        base_name = src_path.stem\n",
    "        label     = r[\"label\"]\n",
    "\n",
    "        out_base  = ATTACK_DIR / r[\"source\"]\n",
    "        out_base.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "        # --- JPEG ---\n",
    "        for q in JPEG_QUALITIES:\n",
    "            attacked = jpeg_attack(img, q)\n",
    "            uid      = uuid.uuid4().hex\n",
    "            out_path = out_base / f\"{uid}.png\"\n",
    "\n",
    "            attacked.save(out_path, optimize = True)\n",
    "\n",
    "            writer.writerow({**r,\n",
    "                             \"id\"        : uid,\n",
    "                             \"filename\"  : str(out_path),\n",
    "                             \"attack\"    : f\"jpeg_q{q}\",\n",
    "                             \"parent_id\" : r[\"id\"]\n",
    "                           })\n",
    "\n",
    "        # --- Resize ---\n",
    "        for s in RESIZE_SCALES:\n",
    "            attacked = resize_attack(img, s)\n",
    "            uid      = uuid.uuid4().hex\n",
    "            out_path = out_base / f\"{uid}.png\"\n",
    "\n",
    "            attacked.save(out_path, optimize = True)\n",
    "\n",
    "            writer.writerow({**r,\n",
    "                             \"id\"        : uid,\n",
    "                             \"filename\"  : str(out_path),\n",
    "                             \"attack\"    : f\"resize_{int(s*100)}\",\n",
    "                             \"parent_id\" : r[\"id\"]\n",
    "                           })\n",
    "\n",
    "        # --- Blur ---\n",
    "        for b in BLUR_RADII:\n",
    "            attacked = blur_attack(img, b)\n",
    "            uid      = uuid.uuid4().hex\n",
    "            out_path = out_base / f\"{uid}.png\"\n",
    "\n",
    "            attacked.save(out_path, optimize = True)\n",
    "\n",
    "            writer.writerow({**r,\n",
    "                             \"id\"        : uid,\n",
    "                             \"filename\"  : str(out_path),\n",
    "                             \"attack\"    : f\"blur_{b}\",\n",
    "                             \"parent_id\" : r[\"id\"]\n",
    "                           })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736496a-7710-4593-86fd-818b2d58d535",
   "metadata": {},
   "source": [
    "## Write Attack Metadata\n",
    "\n",
    "We preserve:\n",
    "- Original label (ai / real)\n",
    "- Source family\n",
    "- Parent image ID\n",
    "- Attack type\n",
    "\n",
    "This allows **per-attack evaluation later**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f29f49-4137-4752-a098-1eba404ce352",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(META_OUT, \"w\", newline = \"\") as f:\n",
    "    fieldnames = list(records[0].keys()) + [\"attack\", \"parent_id\"]\n",
    "    writer     = csv.DictWriter(f, fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    apply_attacks(records, writer)\n",
    "\n",
    "print(\"âœ… Post-processing attacks generated\")\n",
    "print(f\"Metadata saved to: {META_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20b8f36-af23-49b8-8c6b-d93cf2a7ba07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
